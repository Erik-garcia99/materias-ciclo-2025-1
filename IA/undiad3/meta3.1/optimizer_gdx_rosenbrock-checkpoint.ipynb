{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd31ba1-15f9-4c1f-8406-206977b87f9c",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1741565409269,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "abd31ba1-15f9-4c1f-8406-206977b87f9c"
   },
   "outputs": [],
   "source": [
    "# optimizer_gdx_rosenbrock.py\n",
    "# Gradient descent w/momentum & adaptive lr optimization\n",
    "\"\"\"\n",
    "Dr. Juan R. Castro\n",
    "FCQI, UABC Campus Tijuana\n",
    "\"\"\"\n",
    "# Importing Libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51784196-434e-4ea4-a752-e87c0d399d80",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1741565410965,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "51784196-434e-4ea4-a752-e87c0d399d80"
   },
   "outputs": [],
   "source": [
    "# objective Extended Rosenbrock function\n",
    "def objfcn(x):\n",
    "    # Minima -> f=0 at (1,.....,1)\n",
    "    n = len(x) # n par\n",
    "    z = np.zeros((n,1))\n",
    "    l2 = np.array(range(0,n,2)) # indice par\n",
    "    l1 = np.array(range(1,n,2)) # indice impar\n",
    "    z[l2]=10.0*(x[l1]-(x[l2])**2.0)\n",
    "    z[l1]=1.0-x[l2]\n",
    "    f = z.T @ z\n",
    "    return f[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b761d42-bb5c-4ee4-b70b-e8fe3dfe2cd9",
   "metadata": {
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1741565412815,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "4b761d42-bb5c-4ee4-b70b-e8fe3dfe2cd9"
   },
   "outputs": [],
   "source": [
    "# Extended Rosenbrock gradient function\n",
    "def objfcngrad(x):\n",
    "    n = len(x) # n even\n",
    "    Jz = np.zeros((n,n))\n",
    "    z = np.zeros((n,1))\n",
    "    l2 = np.array(range(0,n,2)) # indice par\n",
    "    l1 = np.array(range(1,n,2)) # indice impar\n",
    "    z[l2]=10.0*(x[l1]-(x[l2])**2.0)\n",
    "    z[l1]=1.0-x[l2]\n",
    "\n",
    "    for i in range(n//2):\n",
    "        Jz[2*i,2*i]     = -20.0*x[2*i]\n",
    "        Jz[2*i,2*i+1]   = 10.0\n",
    "        Jz[2*i+1,2*i]   = -1.0\n",
    "\n",
    "    gX = 2.0*Jz.T @ z\n",
    "    return gX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0698dd48-5be8-486a-b7d6-bfd71b2b3777",
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1741565415204,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "0698dd48-5be8-486a-b7d6-bfd71b2b3777"
   },
   "outputs": [],
   "source": [
    "def objfcnjac(x):\n",
    "    # Extended Rosenbrock Jacobian Function\n",
    "    n = len(x) # n even\n",
    "    Jz = np.zeros((n,n))\n",
    "    z = np.zeros((n,1))\n",
    "    l2 = np.array(range(0,n,2)) # indice par\n",
    "    l1 = np.array(range(1,n,2)) # indice impar\n",
    "    z[l2]=10.0*(x[l1]-(x[l2])**2.0)\n",
    "    z[l1]=1.0-x[l2]\n",
    "\n",
    "    for i in range(n//2):\n",
    "        Jz[2*i,2*i]     = -20.0*x[2*i]\n",
    "        Jz[2*i,2*i+1]   = 10.0\n",
    "        Jz[2*i+1,2*i]   = -1.0\n",
    "\n",
    "    gX = 2.0*Jz.T @ z\n",
    "    normgX = np.linalg.norm(gX)\n",
    "    return z, Jz, normgX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98061ca-4262-4071-be9a-4f12d6c5cb6b",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1741565417540,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "a98061ca-4262-4071-be9a-4f12d6c5cb6b"
   },
   "outputs": [],
   "source": [
    "# Gradient descent w/momentum & adaptive lr optimization\n",
    "def traingdx(X,maxEpochs,goal,lr,mc,lr_inc,lr_dec,max_perf_inc,mingrad,show):\n",
    "    # traingdx is a optimization function that updates variables\n",
    "    # values according to gradient descent w/momentum & adaptive lr optimization\n",
    "    this = \"traingdx\"\n",
    "    stop = \"\"\n",
    "    epochs = []\n",
    "    perfs  = []\n",
    "    # Performance and Gradient\n",
    "    perf = objfcn(X)\n",
    "    gX = objfcngrad(X)\n",
    "    normgX = np.linalg.norm(gX)\n",
    "    dX = lr*gX\n",
    "    print(\"\\n\")\n",
    "    # Train\n",
    "    for epoch in range(maxEpochs+1):\n",
    "        # Stopping criteria\n",
    "        if perf <= goal:\n",
    "            stop = \"Performance goal met\"\n",
    "        elif epoch == maxEpochs:\n",
    "            stop = \"Maximum epoch reached, performance goal was not met\"\n",
    "        elif normgX < mingrad:\n",
    "            stop = \"Minimum gradient reached, performance goal was not met\"\n",
    "        # Progress\n",
    "        if (np.fmod(epoch,show) == 0 or len(stop) != 0):\n",
    "            print(this,end = \": \")\n",
    "            if np.isfinite(maxEpochs):\n",
    "                print(\"Epoch \",epoch, \"/\", maxEpochs,end = \" \")\n",
    "            if np.isfinite(goal):\n",
    "                print(\", Performance %8.3e\" % perf, \"/\", goal, end = \" \")\n",
    "            if np.isfinite(mingrad):\n",
    "                print(\", Gradient %8.3e\" % normgX, \"/\", mingrad)\n",
    "\n",
    "            epochs = np.append(epochs,epoch)\n",
    "            perfs = np.append(perfs,perf)\n",
    "            if len(stop) != 0:\n",
    "                print(\"\\n\",this,\":\",stop,\"\\n\")\n",
    "                break\n",
    "        # Gradient Descent with Momentum and Adaptive Learning Rate\n",
    "        dX = mc*dX - (1-mc)*lr*gX\n",
    "        X2 = X + dX\n",
    "        perf2 = objfcn(X2)\n",
    "\n",
    "        if (perf2/perf) > max_perf_inc :\n",
    "            lr = lr*lr_dec\n",
    "            dX = lr*gX\n",
    "        else:\n",
    "            if (perf2 < perf):\n",
    "                lr = lr*lr_inc\n",
    "            X = X2\n",
    "            perf = perf2\n",
    "            gX   = objfcngrad(X)\n",
    "            normgX = np.linalg.norm(gX)\n",
    "\n",
    "    return X, perfs, epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02f5e05-109d-4c01-a528-02593305e6a7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1539,
     "status": "ok",
     "timestamp": 1741565422585,
     "user": {
      "displayName": "erik garcia chavez",
      "userId": "03953579049831948798"
     },
     "user_tz": 420
    },
    "id": "a02f5e05-109d-4c01-a528-02593305e6a7",
    "outputId": "840eb564-fd02-4df5-9f0f-1c60ad9b75cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [[ 16.24345364]\n",
      " [ -6.11756414]\n",
      " [ -5.28171752]\n",
      " [-10.72968622]\n",
      " [  8.65407629]\n",
      " [-23.01538697]\n",
      " [ 17.44811764]\n",
      " [ -7.61206901]\n",
      " [  3.19039096]\n",
      " [ -2.49370375]] \n",
      "\n",
      "\n",
      "\n",
      "traingdx: Epoch  0 / 5000 , Performance 1.815e+07 / 1e-08 , Gradient 2.819e+06 / 1e-11\n",
      "traingdx: Epoch  100 / 5000 , Performance 2.670e+05 / 1e-08 , Gradient 7.422e+04 / 1e-11\n",
      "traingdx: Epoch  200 / 5000 , Performance 7.651e+03 / 1e-08 , Gradient 1.749e+03 / 1e-11\n",
      "traingdx: Epoch  300 / 5000 , Performance 2.377e+00 / 1e-08 , Gradient 2.625e+00 / 1e-11\n",
      "traingdx: Epoch  400 / 5000 , Performance 2.760e-01 / 1e-08 , Gradient 5.742e-01 / 1e-11\n",
      "traingdx: Epoch  500 / 5000 , Performance 9.311e-02 / 1e-08 , Gradient 1.527e+00 / 1e-11\n",
      "traingdx: Epoch  600 / 5000 , Performance 3.825e-02 / 1e-08 , Gradient 1.058e+00 / 1e-11\n",
      "traingdx: Epoch  700 / 5000 , Performance 1.931e-02 / 1e-08 , Gradient 1.857e+00 / 1e-11\n",
      "traingdx: Epoch  800 / 5000 , Performance 1.400e-02 / 1e-08 , Gradient 1.105e-01 / 1e-11\n",
      "traingdx: Epoch  900 / 5000 , Performance 7.809e-03 / 1e-08 , Gradient 9.712e-02 / 1e-11\n",
      "traingdx: Epoch  1000 / 5000 , Performance 4.034e-03 / 1e-08 , Gradient 1.781e-01 / 1e-11\n",
      "traingdx: Epoch  1100 / 5000 , Performance 2.166e-03 / 1e-08 , Gradient 2.748e-01 / 1e-11\n",
      "traingdx: Epoch  1200 / 5000 , Performance 1.029e-03 / 1e-08 , Gradient 2.067e-01 / 1e-11\n",
      "traingdx: Epoch  1300 / 5000 , Performance 5.802e-04 / 1e-08 , Gradient 2.217e-02 / 1e-11\n",
      "traingdx: Epoch  1400 / 5000 , Performance 4.603e-04 / 1e-08 , Gradient 1.952e-02 / 1e-11\n",
      "traingdx: Epoch  1500 / 5000 , Performance 2.592e-04 / 1e-08 , Gradient 2.494e-02 / 1e-11\n",
      "traingdx: Epoch  1600 / 5000 , Performance 1.438e-04 / 1e-08 , Gradient 1.066e-01 / 1e-11\n",
      "traingdx: Epoch  1700 / 5000 , Performance 8.116e-05 / 1e-08 , Gradient 9.459e-02 / 1e-11\n",
      "traingdx: Epoch  1800 / 5000 , Performance 4.258e-05 / 1e-08 , Gradient 3.193e-02 / 1e-11\n",
      "traingdx: Epoch  1900 / 5000 , Performance 2.463e-05 / 1e-08 , Gradient 6.014e-02 / 1e-11\n",
      "traingdx: Epoch  2000 / 5000 , Performance 2.102e-05 / 1e-08 , Gradient 4.141e-03 / 1e-11\n",
      "traingdx: Epoch  2100 / 5000 , Performance 1.144e-05 / 1e-08 , Gradient 3.494e-03 / 1e-11\n",
      "traingdx: Epoch  2200 / 5000 , Performance 6.477e-06 / 1e-08 , Gradient 4.581e-03 / 1e-11\n",
      "traingdx: Epoch  2300 / 5000 , Performance 3.545e-06 / 1e-08 , Gradient 1.780e-03 / 1e-11\n",
      "traingdx: Epoch  2400 / 5000 , Performance 1.971e-06 / 1e-08 , Gradient 1.778e-03 / 1e-11\n",
      "traingdx: Epoch  2500 / 5000 , Performance 1.147e-06 / 1e-08 , Gradient 1.171e-02 / 1e-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikG\\AppData\\Local\\Temp\\ipykernel_4412\\2752700112.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  Jz[2*i,2*i]     = -20.0*x[2*i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traingdx: Epoch  2600 / 5000 , Performance 5.892e-07 / 1e-08 , Gradient 7.123e-03 / 1e-11\n",
      "traingdx: Epoch  2700 / 5000 , Performance 4.818e-07 / 1e-08 , Gradient 6.209e-04 / 1e-11\n",
      "traingdx: Epoch  2800 / 5000 , Performance 2.913e-07 / 1e-08 , Gradient 5.433e-04 / 1e-11\n",
      "traingdx: Epoch  2900 / 5000 , Performance 1.463e-07 / 1e-08 , Gradient 1.969e-03 / 1e-11\n",
      "traingdx: Epoch  3000 / 5000 , Performance 7.953e-08 / 1e-08 , Gradient 1.342e-03 / 1e-11\n",
      "traingdx: Epoch  3100 / 5000 , Performance 4.760e-08 / 1e-08 , Gradient 2.392e-04 / 1e-11\n",
      "traingdx: Epoch  3200 / 5000 , Performance 3.724e-08 / 1e-08 , Gradient 1.727e-04 / 1e-11\n",
      "traingdx: Epoch  3300 / 5000 , Performance 2.174e-08 / 1e-08 , Gradient 1.482e-04 / 1e-11\n",
      "traingdx: Epoch  3400 / 5000 , Performance 1.254e-08 / 1e-08 , Gradient 1.012e-04 / 1e-11\n",
      "traingdx: Epoch  3444 / 5000 , Performance 9.983e-09 / 1e-08 , Gradient 8.933e-05 / 1e-11\n",
      "\n",
      " traingdx : Performance goal met \n",
      "\n",
      "[[0.99995538]\n",
      " [0.99991058]\n",
      " [0.99995535]\n",
      " [0.99991053]\n",
      " [0.99995534]\n",
      " [0.99991051]\n",
      " [0.99995536]\n",
      " [0.99991053]\n",
      " [0.99995533]\n",
      " [0.99991048]]\n"
     ]
    }
   ],
   "source": [
    "# seed the pseudo random number generator\n",
    "np.random.seed(1)\n",
    "n = 10\n",
    "x = 10*(np.random.randn(n,1))\n",
    "print(\"\\n\",x,\"\\n\")\n",
    "# Performance goal met\n",
    "goal = 1e-8\n",
    "# define the total iterations\n",
    "max_epochs = 5000\n",
    "# rate learning\n",
    "lr = 1e-3\n",
    "# momentum\n",
    "mc = 0.9\n",
    "# rate learning increment\n",
    "lr_inc = 1.05\n",
    "# rate learning decrement\n",
    "lr_dec = 0.70\n",
    "# maximum performance increment\n",
    "max_perf_inc = 1.04\n",
    "# minimum gradient\n",
    "min_grad = 1e-11\n",
    "# show\n",
    "show = 100\n",
    "# perform the gradient descent\n",
    "x, perfs, epochs = traingdx(x,max_epochs,goal,lr,mc,lr_inc,lr_dec,max_perf_inc,min_grad,show)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c7b4c6-ac3b-4ad4-a6a7-b20e647f6a32",
   "metadata": {
    "id": "51c7b4c6-ac3b-4ad4-a6a7-b20e647f6a32"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
