{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9a44f1-9241-4d1f-a7a1-8f5a82ff04f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split \u001b[38;5;66;03m#serpara los datos \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report \u001b[38;5;66;03m#estos los regulatado las claisigfacion y la matriz de confusion\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler \u001b[38;5;66;03m#normalizar los datos \u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #serpara los datos \n",
    "from sklearn.metrics import confusion_matrix, classification_report #estos los regulatado las claisigfacion y la matriz de confusion\n",
    "from sklearn.preprocessing import StandardScaler #normalizar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030cbb2-9dd2-4f9b-9039-5a6b72eaefb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis = 1) #etiqeuta en la columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15129ed0-78c8-4ec6-a991-0c794442246b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y susecion de numeros, \n",
    "def one_hot_encode(y):\n",
    "    n_class = np.unique(y).shape[0] #cunatas clases tiene Y \n",
    "    y_encode = np.zeros((y.shape[0], n_class)) #inicialzia la tabla, rengloes(instancias ) columnas numero de clases\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf09186d-11aa-4c78-8987-6ea91fb59de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc #presicion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6016638-ef10-4bec-a9fd-70b58865f224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # self.b0 = None\n",
    "        self.theta = None\n",
    "        \n",
    "    \n",
    "    def _sigmoid(self, A, theta ):\n",
    "        # Linear model: yh = A * theta\n",
    "        yh = np.dot(A, theta) #hipoetsis \n",
    "        # Sigmoid function 1 / (1 + e^(-yh))\n",
    "        return 1/(1 + np.exp(-yh)) #funcion logistica \n",
    "    \n",
    "    #fiunciond e cosot binaria \n",
    "    def _loss(self, y, h):\n",
    "        '''\n",
    "        a really small value 'epsilon' is added to avoid \n",
    "        overflow and divison by zero error for log\n",
    "        loss = (-1/q) * sum(y * log(h) + (1-y) * log(1 - h))\n",
    "        where h = 1/(1 + e^(-yh))\n",
    "        '''\n",
    "        #el epsislon funciona por si la hipotrsis nos da 0 (log -> logaritmo natural )\n",
    "        epsilon = 1e-5\n",
    "        los = (-1/len(y)) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h+epsilon))\n",
    "        return los\n",
    "\n",
    "    def fit(self, A, y, learning_rate=0.01, epochs=100, batch_size=32, show_step = 10, verbose=False):\n",
    "        n_obs = A.shape[0] \n",
    "        batch_loss = []\n",
    "        epoch_loss = []\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.theta = np.zeros(A.shape[1]) #valor de tetca inica ceros. 0 renglosn daot, columan numero de parametros \n",
    "        \n",
    "        for e in range(epochs+1):                \n",
    "            loss_e = 0\n",
    "            \n",
    "            for i in range(0, n_obs, batch_size):\n",
    "                # Subset data for batches\n",
    "                A_batch = A[i:i+batch_size] #matriz de disenio para el minilote \n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                # Calculate loss\n",
    "                y_pred = self._sigmoid(A_batch, self.theta)   # predictions\n",
    "                loss = self._loss(y_batch, y_pred) #perdida \n",
    "                loss_e += loss #acumular la perdida por cada minilote\n",
    "                batch_loss.append(loss) #se agrega a la lista \n",
    "                \n",
    "                \n",
    "                # Calculate gradients\n",
    "                grad = (1/len(A_batch)) * np.dot(y_pred - y_batch, A_batch)\n",
    "                \n",
    "                #empieza el algritmos \n",
    "                # Update parameters\n",
    "                self.theta -= learning_rate * grad\n",
    "                #el optimizador devuelve los valores de tetha optmizados \n",
    "\n",
    "\n",
    "            epoch_loss.append(loss_e)\n",
    "            if verbose==True and e%show_step ==0:\n",
    "                print(f'Epoch: {e}, MBCE Loss: {loss_e}')\n",
    "                \n",
    "        return self.theta, batch_loss, epoch_loss\n",
    "                \n",
    "    def predict(self, A, threshold):\n",
    "        y_predicted = self._sigmoid(A, self.theta)  #make prediction\n",
    "        # Assign prediction to a class: \n",
    "        # if pred >= threshold then 1 else 0 and return as an array\n",
    "        y_predicted_cls = [1 if i >= threshold else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5be09b7-74ad-4f20-93a7-ea758d1d36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = np.loadtxt('admisiones_dataset.txt',delimiter=',')\n",
    "inputs = data[:,0:2]\n",
    "idx = 2-data[:,2] #restamos el 1 para establecer el categorico, adminitivos - 1 no admitivos - 0 \n",
    "targets = np.array(idx, dtype=int)     # codificacion categorica\n",
    "# targets = one_hot_encode(labels)      # one hot encode to classlabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fd61d-038c-4d74-b894-847d661b693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "x_train,x_test,y_train,y_test = train_test_split(inputs,targets,test_size=0.40,random_state=1234) # test_size genreta entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79b0383-688d-420f-ae2b-a7ead4e7bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizar \n",
    "# Standardize the data\n",
    "scaler  = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test  = scaler.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599709aa-dfc7-42d4-a18b-70aaddd6d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrices de disenio, \n",
    "A_train = np.c_[np.ones(len(x_train)), x_train]\n",
    "A_test  = np.c_[np.ones(len(x_test)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e84170-e4f1-4383-b3f8-cd67b67bc78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, MBCE Loss: 4.1315355671415395\n",
      "Epoch: 500, MBCE Loss: 1.0587114401594495\n",
      "Epoch: 1000, MBCE Loss: 0.8593096827207356\n",
      "Epoch: 1500, MBCE Loss: 0.7721679129530061\n",
      "Epoch: 2000, MBCE Loss: 0.7215413101498962\n",
      "Epoch: 2500, MBCE Loss: 0.6880202653128631\n",
      "Epoch: 3000, MBCE Loss: 0.6640494262916445\n",
      "Epoch: 3500, MBCE Loss: 0.6460068065693545\n",
      "Epoch: 4000, MBCE Loss: 0.6319181104045496\n",
      "Epoch: 4500, MBCE Loss: 0.6206079359029666\n",
      "Epoch: 5000, MBCE Loss: 0.6113296611515583\n"
     ]
    }
   ],
   "source": [
    "# Build and fit best LR model\n",
    "alpha = 0.01 #lr\n",
    "maxEpochs = 5000\n",
    "batch = 10 #minilotes\n",
    "show = 500 #view\n",
    "\n",
    "# Build model\n",
    "log_model = Logistic_Regression()\n",
    "# Fit Model\n",
    "theta, batch_loss, epoch_loss = log_model.fit(A_train, y_train, learning_rate=alpha, \n",
    "                                epochs=maxEpochs, batch_size=batch, show_step = show, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "081069de-6d71-4912-aea2-d9d708d5a450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "train_pred = log_model.predict(A_train, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076033ee-edbb-4678-9471-b7465069a30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "train_acc = accuracy(y_train, train_pred)\n",
    "print(f'Accuracy on training set: {train_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c65e9-69de-40fe-91b7-9b343f7d108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training set:\n",
      "\n",
      "Confusion Matrix:\n",
      " [[33  1]\n",
      " [ 1 25]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        34\n",
      "           1       0.96      0.96      0.96        26\n",
      "\n",
      "    accuracy                           0.97        60\n",
      "   macro avg       0.97      0.97      0.97        60\n",
      "weighted avg       0.97      0.97      0.97        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics - son de entrenamiento \n",
    "cm_train = confusion_matrix(y_train, train_pred)\n",
    "train_report = classification_report(y_train, train_pred)\n",
    "\n",
    "print(\"Performance on training set:\\n\")\n",
    "print(f'Confusion Matrix:\\n {cm_train}\\n')\n",
    "print(f'Classification Report:\\n {train_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150ab83-9522-42cb-82c6-171208937a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#los datos de prueba no usados para entrenamiento \n",
    "# Create predictions on test set\n",
    "test_pred = log_model.predict(A_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1438abe-d44c-40db-b084-0c533b53ca15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "test_acc = accuracy(y_test, test_pred)\n",
    "print(f'Accuracy on test set: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f73fdc-cac7-44e3-a7a1-904615bfcfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on test set:\n",
      "\n",
      "Confusion Matrix:\n",
      " [[23  3]\n",
      " [ 5  9]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85        26\n",
      "           1       0.75      0.64      0.69        14\n",
      "\n",
      "    accuracy                           0.80        40\n",
      "   macro avg       0.79      0.76      0.77        40\n",
      "weighted avg       0.80      0.80      0.80        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics\n",
    "cm_test = confusion_matrix(y_test, test_pred)\n",
    "test_report = classification_report(y_test, test_pred)\n",
    "\n",
    "print(\"Performance on test set:\\n\")\n",
    "print(f'Confusion Matrix:\\n {cm_test}\\n')\n",
    "print(f'Classification Report:\\n {test_report}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
