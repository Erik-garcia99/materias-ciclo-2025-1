{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dbc35a-f983-47ea-8a73-233d1524d340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import math \n",
    "#agregacion\n",
    "from itertools import combinations_with_replacement\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07afd55-5c20-433f-a04d-cc54127fd882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(A, THETA):\n",
    "    \"\"\"\n",
    "    Multivariate Linear Regression Model, Yh = A*THETA\n",
    "    The matrix A is sometimes called the design matrix.\n",
    "    \"\"\"\n",
    "    return A.dot(THETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32097b3d-5992-4364-982a-9a0c3a1c4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design Matrix Modificacion\n",
    "\"\"\"\n",
    "def designMatrix(Tau,X):\n",
    "    q,_ = X.shape\n",
    "    for p in range(q):\n",
    "        M = powerVector(Tau,X[p,:])\n",
    "        if p == 0:\n",
    "            A = M\n",
    "            continue\n",
    "        A = np.vstack((A,M))\n",
    "    return A\"\"\"\n",
    "\n",
    "#funcion modificada\n",
    "\n",
    "\n",
    "# def designMatrix(tau, X):\n",
    "#     \"\"\"Matriz de diseño con términos polinómicos\"\"\"\n",
    "#     q, n = X.shape\n",
    "#     A_list = []\n",
    "#     for p in range(q):\n",
    "#         row = X[p, :]\n",
    "#         poly_terms = generate_polynomial_terms(row, tau)\n",
    "#         A_list.append(poly_terms)\n",
    "#     return np.vstack(A_list)\n",
    "\n",
    "def designMatrix(tau, X):\n",
    "    n_samples, n_features = X.shape\n",
    "    n_terms = polyParamsNumber(n_features, tau)\n",
    "    A = np.empty((n_samples, n_terms))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        terms = generate_polynomial_terms(X[i], tau)\n",
    "        terms = np.nan_to_num(terms, nan=0.0, posinf=1e10, neginf=-1e10)\n",
    "        A[i] = terms\n",
    "    \n",
    "    return A\n",
    "\n",
    "\n",
    "\n",
    "# Power Vector M\n",
    "# def powerVector(Tau, V):\n",
    "#     if V.size == 0 or Tau == 0:\n",
    "#         return np.array([1.0])\n",
    "#     Z = V[:-1]\n",
    "#     W = V[-1]\n",
    "#     terms = []\n",
    "#     for k in range(Tau + 1):\n",
    "#         sub_terms = powerVector(Tau - k, Z)\n",
    "#         for term in sub_terms:\n",
    "#             terms.append(term * (W ** k))\n",
    "#     return np.array(terms)\n",
    "\n",
    "#este tennemos con el engine y funciona \n",
    "\"\"\"\n",
    "def powerVector(Tau, V):\n",
    "    if V.size == 0 or Tau == 0:\n",
    "        return np.array([[1.0]])  # Asegurar 2D\n",
    "    Z = V[:-1]\n",
    "    W = V[-1]\n",
    "    terms = []\n",
    "    for k in range(Tau + 1):\n",
    "        sub_terms = powerVector(Tau - k, Z)\n",
    "        for term in sub_terms.flatten():  # Manejar sub_terms 2D\n",
    "            terms.append(term * (W ** k))\n",
    "    return np.array([terms])  # Devolver como 2D (1, n_terms)\n",
    "\"\"\"\n",
    "\n",
    "# #modificacion - agregacio \n",
    "# def powerVector(Tau, V):\n",
    "#     \"\"\"Genera términos polinomiales multivariados de grado <= Tau de forma iterativa.\"\"\"\n",
    "#     n = V.size\n",
    "#     terms = []\n",
    "#     # Generar todas las combinaciones de exponentes para las n variables\n",
    "#     exponents = itertools.product(range(Tau + 1), repeat=n)\n",
    "#     for exp in exponents:\n",
    "#         if sum(exp) <= Tau:  # Solo términos hasta el grado Tau\n",
    "#             term = 1.0\n",
    "#             for i in range(n):\n",
    "#                 term *= V[i] ** exp[i]\n",
    "#             terms.append(term)\n",
    "#     return np.array([terms])  # Devolver como array 2D (1, n_terms)\n",
    "\n",
    "\n",
    "#Generar términos polinómicos de manera iterativa -> esta sustituyendo a powerVector\n",
    "# def generate_polynomial_terms(V, degree):\n",
    "#     \"\"\"Genera términos polinómicos multivariados hasta el grado especificado\"\"\"\n",
    "#     n_features = len(V)\n",
    "#     terms = [1.0]  # Término constante\n",
    "    \n",
    "#     for deg in range(1, degree + 1):\n",
    "#         for indices in combinations_with_replacement(range(n_features), deg):\n",
    "#             term = 1.0\n",
    "#             for idx in indices:\n",
    "#                 term *= V[idx]\n",
    "#             terms.append(term)\n",
    "    \n",
    "#     return np.array(terms)\n",
    "\n",
    "\n",
    "\n",
    "def generate_polynomial_terms(V, degree):\n",
    "    \"\"\"Genera términos polinómicos multivariados hasta el grado especificado\"\"\"\n",
    "    n_features = len(V)\n",
    "    terms = [1.0]  # Término constante\n",
    "    \n",
    "    # Generar términos de grado 1\n",
    "    if degree >= 1:\n",
    "        terms.extend(V)\n",
    "    \n",
    "    # Generar términos de grado 2 y superiores\n",
    "    if degree >= 2:\n",
    "        # Términos cuadráticos puros\n",
    "        terms.extend(V[i]*V[i] for i in range(n_features))\n",
    "        \n",
    "        # Términos cruzados\n",
    "        for i in range(n_features):\n",
    "            for j in range(i+1, n_features):\n",
    "                terms.append(V[i] * V[j])\n",
    "    \n",
    "    return np.array(terms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ac1a61-a261-4dd1-97fe-ce8ce3643e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_theta(rho, n_outputs):\n",
    "    # Inicialización Xavier/Glorot\n",
    "    stdv = 1. / np.sqrt(rho)\n",
    "    return np.random.uniform(-stdv, stdv, size=(rho, n_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3992cd-a9fa-4e1a-b5f2-735360f3846f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial parameter number\n",
    "# def polyParamsNumber(n,tau):\n",
    "#     s = 0\n",
    "#     for l in range(tau+1):\n",
    "#         #val = np.math.factorial(l+n-1)/np.math.factorial(n-1)\n",
    "#         val = math.factorial(l + n - 1) / (math.factorial(n - 1) * math.factorial(l))\n",
    "#         val = val/np.math.factorial(l)\n",
    "#         s = s + val\n",
    "#     return int(s)\n",
    "\n",
    "\n",
    "# def polyParamsNumber(n,tau):\n",
    "#     s = 0\n",
    "#     for l in range(tau+1):\n",
    "#         val = math.factorial(l + n - 1) / (math.factorial(n - 1) * math.factorial(l))\n",
    "#         val = val / math.factorial(l)  # Usar math en lugar de np.math\n",
    "#         s += val\n",
    "#     return int(s)\n",
    "\n",
    "def polyParamsNumber(n, tau):\n",
    "    \"\"\"Calcula eficientemente el número de parámetros\"\"\"\n",
    "    # Fórmula cerrada para grados bajos\n",
    "    if tau == 1:\n",
    "        return n + 1\n",
    "    elif tau == 2:\n",
    "        return (n * (n + 1)) // 2 + n + 1\n",
    "    else:\n",
    "        # Método general para grados más altos (aunque no se recomienda para tau > 2)\n",
    "        return sum(math.comb(n + d - 1, d) for d in range(tau + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac034ab3-c2b4-46c6-9176-9add0b0a5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(Y_true, Y_pred, THETA, lambda_param):\n",
    "#     \"\"\" Mean Squared Error \"\"\"\n",
    "#     E = Y_true - Y_pred\n",
    "#     SSE = np.square(np.linalg.norm(E, 'fro'))\n",
    "#     Reg = (lambda_param/(2*E.shape[0]))*np.square(np.linalg.norm(THETA[1:,:], 'fro'))\n",
    "#     MSE = SSE/(2*E.shape[0]) + Reg\n",
    "#     return MSE\n",
    "\n",
    "def loss(Y_true, Y_pred, THETA, lambda_param):\n",
    "    E = Y_true - Y_pred\n",
    "    SSE = np.square(np.linalg.norm(E, 'fro'))\n",
    "    n = E.shape[0]\n",
    "    if n == 0:  # Evitar división por cero\n",
    "        n = 1\n",
    "    Reg = (lambda_param / (2 * n)) * np.square(np.linalg.norm(THETA[1:, :], 'fro'))\n",
    "    MSE = SSE / (2 * n) + Reg\n",
    "    return MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4872c7f4-d972-4380-93d5-316e1a447c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def gradient(A,E,THETA,lambda_param):\n",
    "#     \"\"\" MSE Gradient \"\"\"\n",
    "#     SSEGrad = -1.0*(A.T).dot(E)\n",
    "#     MSEGrad = SSEGrad/E.shape[0]\n",
    "#     #modificacion en la condicion\n",
    "#     if THETA.shape[0] > 1:  # Evitar error si no hay parámetros para regularizar\n",
    "#         MSEGrad[1:,:] += (lambda_param / A.shape[0]) * THETA[1:,:]\n",
    "#     return MSEGrad\n",
    "\n",
    "def gradient(A, E, THETA, lambda_param):\n",
    "    n_samples = A.shape[0]\n",
    "    if n_samples == 0:  # Evitar división por cero\n",
    "        n_samples = 1\n",
    "    \n",
    "    SSEGrad = -1.0 * (A.T).dot(E)\n",
    "    MSEGrad = SSEGrad / n_samples\n",
    "    \n",
    "    if THETA.shape[0] > 1:\n",
    "        # Solo regularizar si hay suficientes parámetros\n",
    "        MSEGrad[1:, :] += (lambda_param / n_samples) * THETA[1:, :]\n",
    "    \n",
    "    # Clip más estricto para evitar desbordamientos\n",
    "    return np.clip(MSEGrad, -1e2, 1e2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03bbb108-1468-4a53-a342-23a4ba5a8a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GDX\n",
    "\n",
    "# def gdx_optimization(X, Y, tau, lambda_param=0.0, maxEpochs=100, show=10, \n",
    "#                      batch_size=64, learning_rate=0.01, momentum=0.9, \n",
    "#                      lr_dec=0.5, lr_inc=1.05, max_perf_inc=1.04, \n",
    "#                      stopping_threshold=1e-6):\n",
    "    \n",
    "#     A_full = designMatrix(tau, X)\n",
    "#     rho = A_full.shape[1]\n",
    "#     n_features = X.shape[1]\n",
    "#     n_outputs = Y.shape[1]\n",
    "    \n",
    "   \n",
    "    \n",
    "#     THETA = np.random.randn(rho, n_outputs) * 0.01\n",
    "#     delta_THETA = np.zeros((rho, n_outputs))\n",
    "#     lr = learning_rate\n",
    "#     n_samples = X.shape[0]\n",
    "#     previous_loss = np.inf\n",
    "\n",
    "   \n",
    "#     for epoch in range(maxEpochs + 1):\n",
    "#         THETA_prev = THETA.copy()\n",
    "        \n",
    "        \n",
    "#         indices = np.random.permutation(n_samples)\n",
    "#         # X_shuffled = X[indices]\n",
    "#         # Y_shuffled = Y[indices]\n",
    "        \n",
    "#         # Procesamiento por lotes\n",
    "#         for start in range(0, n_samples, batch_size):\n",
    "#             end = min(start + batch_size, n_samples)\n",
    "#             # X_batch = X_shuffled[start:end]\n",
    "#             # Y_batch = Y_shuffled[start:end]\n",
    "            \n",
    "#             # 5. Generar matriz de diseño para el lote actual\n",
    "#             # A_batch = designMatrix(tau, X_batch)\n",
    "\n",
    "#             A_batch = A_full[batch_idx]\n",
    "#             Y_batch = Y[batch_idx]\n",
    "            \n",
    "#             # 6. Predicción y cálculo de error\n",
    "#             Y_pred = model(A_batch, THETA)\n",
    "#             E_batch = Y_batch - Y_pred\n",
    "            \n",
    "#             # 7. Cálculo y ajuste del gradiente\n",
    "#             Grad = gradient(A_batch, E_batch, THETA, lambda_param)\n",
    "#             Grad = np.clip(Grad, -1e3, 1e3)  # Prevenir desbordamientos\n",
    "            \n",
    "#             # 8. Actualización de parámetros con momento\n",
    "#             delta_THETA = momentum * delta_THETA - (1 - momentum) * lr * Grad\n",
    "#             THETA += delta_THETA\n",
    "        \n",
    "#         # 9. Cálculo de pérdida completa después de la época\n",
    "#         A_full = designMatrix(tau, X)\n",
    "#         Y_pred_full = model(A_full, THETA)\n",
    "#         current_loss = loss(Y, Y_pred_full, THETA, lambda_param)\n",
    "        \n",
    "#         # 10. Ajuste dinámico de tasa de aprendizaje\n",
    "#         if current_loss > previous_loss * max_perf_inc:\n",
    "#             THETA = THETA_prev\n",
    "#             lr *= lr_dec\n",
    "#         elif current_loss < previous_loss:\n",
    "#             lr *= lr_inc\n",
    "        \n",
    "#         # 11. Monitoreo del progreso\n",
    "#         if epoch % show == 0:\n",
    "#             print(f\"Epoch {epoch}: Loss={current_loss:.3e}, lr={lr:.2e}\")\n",
    "        \n",
    "#         # 12. Criterio de parada temprana\n",
    "#         if abs(previous_loss - current_loss) < stopping_threshold:\n",
    "#             print(f\"Early stopping at epoch {epoch}\")\n",
    "#             break\n",
    "            \n",
    "#         previous_loss = current_loss\n",
    "\n",
    "#     return THETA\n",
    "\n",
    "def gdx_optimization(X, Y, tau, lambda_param=0.0, maxEpochs=100, show=10, \n",
    "                     batch_size=64, learning_rate=0.01, momentum=0.9, \n",
    "                     lr_dec=0.5, lr_inc=1.05, max_perf_inc=1.04, \n",
    "                     stopping_threshold=1e-6):\n",
    " \n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    n_outputs = Y.shape[1]\n",
    "    rho = polyParamsNumber(n_features, tau)\n",
    "    \n",
    "\n",
    "    print(f\"precalculando matriz de disenio (tau={tau}, terminos={rho})...\")\n",
    "    A_full = designMatrix(tau, X)\n",
    "    print(\"matriz de disenio precalculada\")\n",
    "    \n",
    "\n",
    "    THETA = initialize_theta(rho, n_outputs)\n",
    "    delta_THETA = np.zeros((rho, n_outputs))\n",
    "    lr = learning_rate\n",
    "    previous_loss = np.inf\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size  \n",
    "    \n",
    "   \n",
    "    for epoch in range(maxEpochs + 1):\n",
    "        THETA_prev = THETA.copy()\n",
    "        \n",
    "        # Mezclar índices\n",
    "        indices = np.random.permutation(n_samples)\n",
    "        \n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min((batch_idx + 1) * batch_size, n_samples)\n",
    "            idx = indices[start:end]\n",
    "            \n",
    "            \n",
    "            A_batch = A_full[idx]\n",
    "            Y_batch = Y[idx]\n",
    "            \n",
    "          \n",
    "            Y_pred = model(A_batch, THETA)\n",
    "            E_batch = Y_batch - Y_pred\n",
    "            \n",
    "          \n",
    "            Grad = gradient(A_batch, E_batch, THETA, lambda_param)\n",
    "            Grad = np.clip(Grad, -1e3, 1e3)\n",
    "            \n",
    "           \n",
    "            delta_THETA = momentum * delta_THETA - (1 - momentum) * lr * Grad\n",
    "            THETA += delta_THETA\n",
    "        \n",
    "       \n",
    "        Y_pred_full = model(A_full, THETA)\n",
    "        current_loss = loss(Y, Y_pred_full, THETA, lambda_param)\n",
    "        \n",
    "       \n",
    "        if current_loss > previous_loss * max_perf_inc:\n",
    "            THETA = THETA_prev\n",
    "            lr *= lr_dec\n",
    "            print(f\"Reduciendo lr a {lr:.2e}\")\n",
    "        elif current_loss < previous_loss:\n",
    "            lr *= lr_inc\n",
    "    \n",
    "        if epoch % show == 0 or epoch == maxEpochs:\n",
    "            print(f\"Epoch {epoch}: Loss={current_loss:.3e}, lr={lr:.2e}\")\n",
    "        \n",
    "        \n",
    "        if abs(previous_loss - current_loss) < stopping_threshold:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "            \n",
    "        previous_loss = current_loss\n",
    "\n",
    "    return THETA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c7805c-bfd6-444f-aede-4ef19a8d91f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "# mat = sp.loadmat('engine_dataset.mat')\n",
    "# print(mat.keys())\n",
    "# # inputs  = mat['engineInputs'].T\n",
    "# # targets = mat['engineTargets'].T\n",
    "\n",
    "\n",
    "#////////////////////////////////////////////\n",
    "\n",
    "\n",
    "mat = sp.loadmat('challenge04_syntheticdataset22.mat')\n",
    "# print(mat.keys())\n",
    "inputs  = mat['inputs'].T\n",
    "targets = mat['targets'].T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "425892c6-e64d-4e54-982a-24854400942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Test Split Data\n",
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(inputs, targets, random_state = 1, test_size = 0.4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ccc41a-a61e-4049-a261-949072460a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_x = RobustScaler()\n",
    "scaler_t = RobustScaler()\n",
    "xTrain = scaler_x.fit_transform(inputs_train)\n",
    "tTrain = scaler_t.fit_transform(targets_train)\n",
    "xTest = scaler_x.transform(inputs_test)\n",
    "tTest = scaler_t.transform(targets_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67608084-f234-499e-8028-a082e98fed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and Test Data\n",
    "# # Train Data\n",
    "# xTrain = inputs_train\n",
    "# tTrain = targets_train\n",
    "# # Test Data\n",
    "# xTest = inputs_test\n",
    "# tTest = targets_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2146f15-7617-48b8-9c81-f4f6f1370b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precalculando matriz de disenio (tau=1, terminos=10001)...\n",
      "matriz de disenio precalculada\n",
      "Epoch 0: Loss=1.683e+01, lr=1.05e-04\n",
      "Epoch 1: Loss=1.683e+01, lr=1.10e-04\n",
      "Epoch 2: Loss=1.683e+01, lr=1.16e-04\n",
      "Epoch 3: Loss=1.683e+01, lr=1.22e-04\n",
      "Epoch 4: Loss=1.683e+01, lr=1.28e-04\n",
      "Epoch 5: Loss=1.683e+01, lr=1.34e-04\n",
      "Epoch 6: Loss=1.683e+01, lr=1.41e-04\n",
      "Epoch 7: Loss=1.683e+01, lr=1.48e-04\n",
      "Epoch 8: Loss=1.683e+01, lr=1.55e-04\n",
      "Epoch 9: Loss=1.683e+01, lr=1.63e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#mini batch\n",
    "# Find the optimal parameters m and b with RMSprop\n",
    "#polinomio grado 1\n",
    "# tau = 1\n",
    "# lambda_param = 0.1\n",
    "\n",
    "# THETA = gdx_optimization(\n",
    "#     xTrain,\n",
    "#     tTrain,\n",
    "#     tau=tau,\n",
    "#     lambda_param=lambda_param,\n",
    "#     maxEpochs=10000,\n",
    "#     show=500,\n",
    "#     batch_size=256,\n",
    "#     learning_rate=1e-8,\n",
    "#     momentum=0.9,          \n",
    "#     lr_dec=0.5,           \n",
    "#     lr_inc=1.05,\n",
    "#     max_perf_inc=1.04,\n",
    "#     stopping_threshold=1e-6,\n",
    "# )\n",
    "\n",
    "\n",
    "#mini batch\n",
    "# Find the optimal parameters m and b with RMSprop\n",
    "tau = 1\n",
    "lambda_param = 0.01\n",
    "\n",
    "THETA = gdx_optimization(\n",
    "    xTrain,\n",
    "    tTrain,\n",
    "    tau=tau,\n",
    "    lambda_param=lambda_param,\n",
    "    maxEpochs=10,\n",
    "    show=1,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-4,\n",
    "    momentum=0.9,          \n",
    "    lr_dec=0.5,           \n",
    "    lr_inc=1.05,\n",
    "    max_perf_inc=1.04,\n",
    "    stopping_threshold=1e-6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34f73a8-1a35-4501-9b3d-5a38ee8eedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ONLINE (1 a 1)\n",
    "\"\"\"\n",
    "tau = 2\n",
    "lambda_param = 0.001\n",
    "THETA = gdx_optimization(\n",
    "    xTrain,\n",
    "    tTrain,\n",
    "    tau=tau,\n",
    "    lambda_param=lambda_param,\n",
    "    maxEpochs=100000,\n",
    "    show=1000,\n",
    "    batch_size=1,\n",
    "    learning_rate=1e-6,\n",
    "    momentum=0.9,          \n",
    "    lr_dec=0.5,           \n",
    "    lr_inc=1.05,\n",
    "    max_perf_inc=1.04,\n",
    "    stopping_threshold=1e-6,\n",
    ")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c940b1-b767-4ed5-a8db-d9ecef784be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #lote\n",
    "# tau = 1\n",
    "# lambda_param = 0.001\n",
    "# THETA = gdx_optimization(\n",
    "#     xTrain,\n",
    "#     tTrain,\n",
    "#     tau=tau,\n",
    "#     lambda_param=lambda_param,\n",
    "#     maxEpochs=10000,\n",
    "#     show=500,\n",
    "#     batch_size=xTrain.shape[0],\n",
    "#     learning_rate=1e-4,\n",
    "#     momentum=0.9,          \n",
    "#     lr_dec=0.5,           \n",
    "#     lr_inc=1.05,\n",
    "#     max_perf_inc=1.04,\n",
    "#     stopping_threshold=1e-6,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ee52b9-7b72-465d-9a3f-44b33d5e7cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "# Train data\n",
    "A_train = designMatrix(tau, xTrain)\n",
    "A_test = designMatrix(tau, xTest)\n",
    "\n",
    "# Predecir\n",
    "outputTrain_scaled = model(A_train, THETA)\n",
    "outputTest_scaled = model(A_test, THETA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52a068b-31ab-4a3b-8a8f-82846f2ee0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputTrain = scaler_t.inverse_transform(outputTrain_scaled)\n",
    "outputTest = scaler_t.inverse_transform(outputTest_scaled)\n",
    "tTrain_orig = scaler_t.inverse_transform(tTrain)\n",
    "tTest_orig = scaler_t.inverse_transform(tTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc34a1e0-de68-42ad-8b89-c7ab920cabe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 for raw train data\n",
    "# R2_train = r2_score(tTrain_orig.reshape(-1, 1),outputTrain.reshape(-1, 1))\n",
    "R2_train = r2_score(tTrain_orig, outputTrain)\n",
    "print(R2_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5a6797-fcd3-4746-bb20-a6d51d00764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE for raw train data\n",
    "# MSE_train = mean_squared_error(tTrain_orig.reshape(-1, 1),outputTrain.reshape(-1, 1))\n",
    "MSE_train = mean_squared_error(tTrain_orig, outputTrain)\n",
    "print(MSE_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daeb5a-b635-4077-b2f5-65f33fb27090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 for raw test data\n",
    "# R2_test = r2_score(tTest_orig.reshape(-1, 1),outputTest.reshape(-1, 1))\n",
    "R2_test = r2_score(tTest_orig, outputTest)\n",
    "print(R2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a27c9e4-8066-49bd-ab99-0ebd49a214e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MSE for raw test data\n",
    "# MSE_test = mean_squared_error(tTest_orig.reshape(-1, 1),outputTest.reshape(-1, 1))\n",
    "MSE_test = mean_squared_error(tTest_orig, outputTest)\n",
    "print(MSE_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af11b13-58a8-49d2-8008-e0ece4dc4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "THETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16592de1-03f2-408d-9610-45962fdeb5b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26001b23-891b-4313-8210-8780b2936d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
