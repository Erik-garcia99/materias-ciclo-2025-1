{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0e2e34-bd5c-4463-b81b-51f927e2eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #serpara los datos \n",
    "from sklearn.metrics import confusion_matrix, classification_report #estos los regulatado las claisigfacion y la matriz de confusion\n",
    "from sklearn.preprocessing import StandardScaler #normalizar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afd663d-7f48-486b-bc5d-b1b4ceb50bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_classlabel(z):\n",
    "    return z.argmax(axis = 1) #etiqeuta en la columna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93092f02-a4d0-4bbc-94cf-17ffe2d923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y susecion de numeros, \n",
    "def one_hot_encode(y):\n",
    "    n_class = np.unique(y).shape[0] #cunatas clases tiene Y \n",
    "    y_encode = np.zeros((y.shape[0], n_class)) #inicialzia la tabla, rengloes(instancias ) columnas numero de clases\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af40f7-1447-4dc3-be84-ac665a309840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc #presicion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287dc6c-8881-40fe-aed4-376e7cf59b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_GDX():\n",
    "    \n",
    "    def __init__(self, lambda_param=0.01):\n",
    "        self.theta = None\n",
    "        self.lambda_param = lambda_param \n",
    "        \n",
    "    \n",
    "    \"\"\"modificacion\n",
    "    def _sigmoid(self, A, theta ):\n",
    "        # Linear model: yh = A * theta\n",
    "        yh = np.dot(A, theta) #hipoetsis \n",
    "        # Sigmoid function 1 / (1 + e^(-yh))\n",
    "        return 1/(1 + np.exp(-yh)) #funcion logistica \n",
    "    \"\"\"\n",
    "    # def _sigmoid(self, A, theta):\n",
    "    #     yh = np.dot(A, theta)\n",
    "    #     return 1/(1 + np.exp(-yh))\n",
    "\n",
    "    def _sigmoid(self, A, theta):\n",
    "        yh = np.dot(A, theta)\n",
    "    \n",
    "        yh = np.clip(yh, -500, 500)\n",
    "        return 1/(1 + np.exp(-yh))\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    def _loss(self, y, h):\n",
    "        '''\n",
    "        a really small value 'epsilon' is added to avoid \n",
    "        overflow and divison by zero error for log\n",
    "        loss = (-1/q) * sum(y * log(h) + (1-y) * log(1 - h))\n",
    "        where h = 1/(1 + e^(-yh))\n",
    "        '''\n",
    "        epsilon = 1e-5\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        bce = - (y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "        bce_loss = np.mean(bce)\n",
    "        reg_term = (self.lambda_param/(2*len(y))) * np.sum(self.theta[1:]**2)\n",
    "        return bce_loss + reg_term\n",
    "\n",
    "    def fit(self, A, y, learning_rate=0.01, momentum=0.9, \n",
    "        lr_dec=0.5, lr_inc=1.05, max_perf_inc=1.04,\n",
    "        epochs=100, batch_size=32, show_step=10, \n",
    "        stopping_threshold=1e-6, verbose=False):\n",
    "\n",
    "        self.theta = np.random.randn(A.shape[1]) * 0.01   \n",
    "        n_obs = A.shape[0]\n",
    "        batch_loss = []\n",
    "        epoch_loss = []\n",
    "        delta_theta = np.zeros_like(self.theta)\n",
    "        lr = learning_rate\n",
    "        previous_loss = np.inf\n",
    "\n",
    "        for e in range(epochs+1):\n",
    "            THETA_prev = self.theta.copy()\n",
    "            loss_e = 0\n",
    "        \n",
    "        \n",
    "            indices = np.random.permutation(n_obs)\n",
    "            A_shuffled = A[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            \n",
    "        \n",
    "            # Calcular número de lotes completos y residual\n",
    "            n_batches = n_obs // batch_size\n",
    "            residual = n_obs % batch_size\n",
    "            total_batches = n_batches + (1 if residual != 0 else 0)\n",
    "        \n",
    "            for batch_idx in range(total_batches):\n",
    "           \n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                if batch_idx == n_batches and residual != 0:\n",
    "                    end = start + residual\n",
    "                \n",
    "                A_batch = A_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "           \n",
    "\n",
    "                # if fit_params['batch_size'] == len(y_train): \n",
    "                #     dropout_mask = np.random.binomial(1, 0.7, size=A_batch.shape)\n",
    "                #     A_batch = A_batch * dropout_mask\n",
    "            \n",
    "               \n",
    "                y_pred = self._sigmoid(A_batch, self.theta)\n",
    "                loss = self._loss(y_batch, y_pred)\n",
    "                loss_e += loss\n",
    "                batch_loss.append(loss)\n",
    "            \n",
    "             \n",
    "                grad = (1/len(A_batch)) * np.dot(y_pred - y_batch, A_batch)\n",
    "                grad[1:] += (self.lambda_param/len(A_batch)) * self.theta[1:]\n",
    "            \n",
    "                \n",
    "                delta_theta = momentum * delta_theta - (1 - momentum) * lr * grad\n",
    "                self.theta += delta_theta\n",
    "        \n",
    "           \n",
    "            if loss_e > previous_loss * max_perf_inc:\n",
    "                self.theta = THETA_prev\n",
    "                lr *= lr_dec\n",
    "            elif loss_e < previous_loss:\n",
    "                lr *= lr_inc\n",
    "            \n",
    "            epoch_loss.append(loss_e)\n",
    "        \n",
    "          \n",
    "            if abs(previous_loss - loss_e) < stopping_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {e}\")\n",
    "                break\n",
    "            \n",
    "            previous_loss = loss_e\n",
    "        \n",
    "            if verbose and e % show_step == 0:\n",
    "                print(f'Epoch: {e}, Loss: {loss_e:.3e}, lr: {lr:.2e}')\n",
    "            \n",
    "        return self.theta, batch_loss, epoch_loss\n",
    "                \n",
    "    def predict(self, A, threshold):\n",
    "        y_predicted = self._sigmoid(A, self.theta)  #make prediction\n",
    "        # Assign prediction to a class: \n",
    "        # if pred >= threshold then 1 else 0 and return as an array\n",
    "        y_predicted_cls = [1 if i >= threshold else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454139ab-1f46-4170-aa3d-2e8208d759e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_GDX_Multiclass():\n",
    "    def __init__(self, lambda_param=0.01, num_classes=6):\n",
    "        self.models = []\n",
    "        self.losses = []  # Almacenar pérdidas por época\n",
    "        self.lambda_param = lambda_param\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        \n",
    "    def fit(self, A, y, **fit_params):\n",
    "        self.models = []\n",
    "        self.losses = []  # reinicializar en cada entrenamiento\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            y_binary = np.where(y == i, 1, 0)\n",
    "            \n",
    "            positive_indices = np.where(y_binary == 1)[0]\n",
    "            negative_indices = np.where(y_binary == 0)[0]\n",
    "            \n",
    "            # Submuestrear la clase mayoritaria (negativos)\n",
    "            n_pos = len(positive_indices)\n",
    "            \n",
    "            # Verificar si hay muestras positivas\n",
    "            if n_pos > 0:\n",
    "                neg_selected = np.random.choice(negative_indices, size=n_pos, replace=False)\n",
    "                \n",
    "                # combinar indices balanceados\n",
    "                balanced_indices = np.concatenate([positive_indices, neg_selected])\n",
    "                np.random.shuffle(balanced_indices)\n",
    "                \n",
    "                # extraer datos balanceados\n",
    "                A_balanced = A[balanced_indices]\n",
    "                y_balanced = y_binary[balanced_indices]\n",
    "            else:\n",
    "                print(f\"Advertencia: Clase {i} tiene 0 muestras positivas\")\n",
    "                A_balanced = A\n",
    "                y_balanced = y_binary\n",
    "            model = Logistic_Regression_GDX(lambda_param=self.lambda_param)\n",
    "            theta, batch_loss, epoch_loss = model.fit(A_balanced, y_balanced, **fit_params)\n",
    "            self.models.append(model)\n",
    "            self.losses.append(epoch_loss)  \n",
    "    \n",
    "    def predict_proba(self, A):\n",
    "        probas = []\n",
    "        for model in self.models:\n",
    "   \n",
    "            proba = 1 / (1 + np.exp(-np.dot(A, model.theta)))\n",
    "            probas.append(proba)\n",
    "        return np.array(probas).T\n",
    "    \n",
    "    def predict(self, A):\n",
    "        probas = self.predict_proba(A)\n",
    "        return np.argmax(probas, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4cbb5d-a619-4e9f-ad06-e07890694115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Read the data\n",
    "# data = np.loadtxt('admisiones_dataset.txt',delimiter=',')\n",
    "# inputs = data[:,0:2]\n",
    "# idx = 2-data[:,2] #restamos el 1 para establecer el categorico, adminitivos - 1 no admitivos - 0 \n",
    "# targets = np.array(idx, dtype=int)     # codificacion categorica\n",
    "# # targets = one_hot_encode(labels)      # one hot encode to classlabel\n",
    "\n",
    "\n",
    "# Leer datos desde archivo dermatology.dat\n",
    "# Reemplazar la sección de lectura de datos con:\n",
    "try:\n",
    "    # Leer el archivo .dat\n",
    "    df = pd.read_csv('dermatology.dat', header=None, na_values='?', delimiter=r'\\s+')\n",
    "    \n",
    "   \n",
    "    inputs = df.iloc[:, 0:34]\n",
    "    targets = df.iloc[:, 34]\n",
    "    \n",
    "  \n",
    "    age_mean = inputs[33].mean()\n",
    "    inputs[33] = inputs[33].fillna(age_mean)\n",
    "    \n",
    "    # Convertir todo a tipo float\n",
    "    inputs = inputs.astype(float)\n",
    "    targets = targets.astype(int) - 1  # Convertir clases de [1-6] a [0-5]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7cc72c-27bc-40df-8042-7c0837a4be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data\n",
    "# x_train,x_test,y_train,y_test = train_test_split(inputs,targets,test_size=0.40,random_state=1234) # test_size genreta entrenamiento y prueba \n",
    "\n",
    "# División de datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs, targets, test_size=0.3, random_state=42\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d6c9b-8184-4326-b571-0a3b26ab6139",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_= scaler.fit_transform(x_train)\n",
    "x_test= scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b346c0-c891-43e2-a75b-192a7daaa117",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matrices de disenio, \n",
    "A_train = np.c_[np.ones(len(x_train)), x_train]\n",
    "A_test = np.c_[np.ones(len(x_test)), x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa9fa54-0098-4e8a-9e1e-d28f9c439c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Parámetros para el constructor\n",
    "\n",
    "#minilot4es\n",
    "# Hiperparámetros GDX \n",
    "# lambda_param = 0.01  \n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-2,\n",
    "#     'momentum': 0.95,\n",
    "#     'lr_dec': 0.5,\n",
    "#     'lr_inc': 1.05,\n",
    "#     'max_perf_inc': 1.04,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 64,\n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-6,\n",
    "#     'verbose': True\n",
    "# }\n",
    "\n",
    "\n",
    "#online\n",
    "# lambda_param = 0.01 \n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-2,\n",
    "#     'momentum': 0.95,\n",
    "#     'lr_dec': 0.5,\n",
    "#     'lr_inc': 1.05,\n",
    "#     'max_perf_inc': 1.04,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 1,\n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-6,\n",
    "#     'verbose': True\n",
    "# }\n",
    "\n",
    "\n",
    "# #online\n",
    "lambda_param = 0.01 \n",
    "fit_params = {\n",
    "    'learning_rate': 1e-2,\n",
    "    'momentum': 0.95,\n",
    "    'lr_dec': 0.5,\n",
    "    'lr_inc': 1.05,\n",
    "    'max_perf_inc': 1.04,\n",
    "    'epochs': 300,\n",
    "    'batch_size': len(y_train),\n",
    "    'show_step': 50,\n",
    "    'stopping_threshold': 1e-4,\n",
    "    'verbose': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca913b-548c-4ec0-b0e7-c72a72368e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Build and fit best LR model\n",
    "# alpha = 0.01 #lr\n",
    "# maxEpochs = 5000\n",
    "# batch = 10 #minilotes\n",
    "# show = 500 #view\n",
    "\n",
    "# # Build model\n",
    "# log_model = Logistic_Regression()\n",
    "# # Fit Model\n",
    "# theta, batch_loss, epoch_loss = log_model.fit(A_train, y_train, learning_rate=alpha, \n",
    "#                                 epochs=maxEpochs, batch_size=batch, show_step = show, verbose=True)\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "# Entrenamiento del modelo dermatology.dat\n",
    "model = Logistic_Regression_GDX_Multiclass(lambda_param=lambda_param, num_classes=6)\n",
    "model.fit(A_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8fd2bb-d987-4c36-b2bb-d92e815ce6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(loss_list) for loss_list in model.losses)\n",
    "losses_array = np.full((len(model.losses), max_length), np.nan)\n",
    "for i, loss_list in enumerate(model.losses):\n",
    "    losses_array[i, :len(loss_list)] = loss_list\n",
    "avg_epoch_loss = np.nanmean(losses_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da415d-83e9-4504-98e3-034b6f128eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicciones y evaluación\n",
    "# Predicciones\n",
    "train_pred = model.predict(A_train)\n",
    "test_pred = model.predict(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07115e39-11f9-4401-ae5c-c7a9866345bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"rendimiento en entrenamiento (multiclase):\")\n",
    "print(f'Accuracy: {accuracy(y_train, train_pred):.4f}')\n",
    "print(\"\\nmatriz de confusion:\")\n",
    "print(confusion_matrix(y_train, train_pred))\n",
    "print(\"\\nreporte de clasificacion:\")\n",
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaca170-50fd-4a3e-9f0a-eeae44f1f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nrendimiento en prueba (multiclase):\")\n",
    "print(f'Accuracy: {accuracy(y_test, test_pred):.4f}')\n",
    "print(\"\\nmatriz de confusion:\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(\"\\nreporte de clasificacion:\")\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcbceb-8256-471a-af34-b0a2a9e4ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Gráfica de pérdida\n",
    "# Gráfica de pérdida\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.plot(avg_epoch_loss, 'b-', linewidth=2)  # Usar avg_epoch_loss\n",
    "# plt.title('Evolución de la Pérdida por Época')\n",
    "# plt.xlabel('Época')\n",
    "# plt.ylabel('Pérdida (Entropía Cruzada Binaria)')\n",
    "# plt.grid(True)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, loss_list in enumerate(model.losses):\n",
    "    plt.plot(loss_list, label=f'Clase {i}')\n",
    "plt.title('evolucion de la perdida por epoca (GDX Multiclase)')\n",
    "plt.xlabel('epoca')\n",
    "plt.ylabel('perdida (Entropia cruzada binaria)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
