{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b610a36-ae17-4b19-9f6a-242443a4cd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #serpara los datos \n",
    "from sklearn.metrics import confusion_matrix, classification_report #estos los regulatado las claisigfacion y la matriz de confusion\n",
    "from sklearn.preprocessing import StandardScaler #normalizar los datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53298880-173a-4747-8f4b-c4651fa1a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis = 1) #etiqeuta en la columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e7b62b-811d-4a85-bc51-7c6aa34deb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y susecion de numeros, \n",
    "def one_hot_encode(y):\n",
    "    n_class = np.unique(y).shape[0] #cunatas clases tiene Y \n",
    "    y_encode = np.zeros((y.shape[0], n_class)) #inicialzia la tabla, rengloes(instancias ) columnas numero de clases\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcbee6c-12f7-43cc-b454-90831794a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc #presicion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a96eb0c-1e7f-443f-a8dc-2a273e98c5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_GDX():\n",
    "    \n",
    "    def __init__(self, lambda_param=0.01):\n",
    "        self.theta = None\n",
    "        self.lambda_param = lambda_param  # Parámetro de regularización L2\n",
    "        \n",
    "    \n",
    "    \"\"\"modificacion\n",
    "    def _sigmoid(self, A, theta ):\n",
    "        # Linear model: yh = A * theta\n",
    "        yh = np.dot(A, theta) #hipoetsis \n",
    "        # Sigmoid function 1 / (1 + e^(-yh))\n",
    "        return 1/(1 + np.exp(-yh)) #funcion logistica \n",
    "    \"\"\"\n",
    "    # def _sigmoid(self, A, theta):\n",
    "    #     yh = np.dot(A, theta)\n",
    "    #     return 1/(1 + np.exp(-yh))\n",
    "\n",
    "    def _sigmoid(self, A, theta):\n",
    "        yh = np.dot(A, theta)\n",
    "        # Añadir clip para estabilidad numérica\n",
    "        yh = np.clip(yh, -500, 500)\n",
    "        return 1/(1 + np.exp(-yh))\n",
    "\n",
    "\n",
    "\n",
    "    #fiunciond e cosot binaria \n",
    "    def _loss(self, y, h):\n",
    "        '''\n",
    "        a really small value 'epsilon' is added to avoid \n",
    "        overflow and divison by zero error for log\n",
    "        loss = (-1/q) * sum(y * log(h) + (1-y) * log(1 - h))\n",
    "        where h = 1/(1 + e^(-yh))\n",
    "        '''\n",
    "        #el epsislon funciona por si la hipotrsis nos da 0 (log -> logaritmo natural )\n",
    "        epsilon = 1e-5\n",
    "        # modificacion \n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        # Pérdida con regularización L2\n",
    "        reg_term = (self.lambda_param/(2*len(y))) * np.sum(self.theta[1:]**2)\n",
    "        los = (-1/len(y)) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h+epsilon)) + reg_term\n",
    "        return los\n",
    "\n",
    "    def fit(self, A, y, learning_rate=0.01, momentum=0.9, \n",
    "        lr_dec=0.5, lr_inc=1.05, max_perf_inc=1.04,\n",
    "        epochs=100, batch_size=32, show_step=10, \n",
    "        stopping_threshold=1e-6, verbose=False):\n",
    "\n",
    "        self.theta = np.random.randn(A.shape[1]) * 0.01\n",
    "        n_obs = A.shape[0]\n",
    "        batch_loss = []\n",
    "        epoch_loss = []\n",
    "        # self.theta = np.zeros(A.shape[1])\n",
    "        delta_theta = np.zeros_like(self.theta)\n",
    "        lr = learning_rate\n",
    "        previous_loss = np.inf\n",
    "\n",
    "        for e in range(epochs+1):\n",
    "            THETA_prev = self.theta.copy()\n",
    "            loss_e = 0\n",
    "        \n",
    "            # Barajar datos cada época\n",
    "            indices = np.random.permutation(n_obs)\n",
    "            A_shuffled = A[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            \n",
    "        \n",
    "            # Calcular número de lotes completos y residual\n",
    "            n_batches = n_obs // batch_size\n",
    "            residual = n_obs % batch_size\n",
    "            total_batches = n_batches + (1 if residual != 0 else 0)\n",
    "        \n",
    "            for batch_idx in range(total_batches):\n",
    "                # Calcular índices del lote actual\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                if batch_idx == n_batches and residual != 0:\n",
    "                    end = start + residual\n",
    "                \n",
    "                A_batch = A_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "\n",
    "                #modiciacion: para cunado es de lotes\n",
    "\n",
    "                if fit_params['batch_size'] == len(y_train):  # Solo para batch completo\n",
    "                    dropout_mask = np.random.binomial(1, 0.7, size=A_batch.shape)\n",
    "                    A_batch = A_batch * dropout_mask\n",
    "            \n",
    "                # Calcular predicción y pérdida\n",
    "                y_pred = self._sigmoid(A_batch, self.theta)\n",
    "                loss = self._loss(y_batch, y_pred)\n",
    "                loss_e += loss\n",
    "                batch_loss.append(loss)\n",
    "            \n",
    "                # Calcular gradiente con regularización L2\n",
    "                grad = (1/len(A_batch)) * np.dot(y_pred - y_batch, A_batch)\n",
    "                grad[1:] += (self.lambda_param/len(A_batch)) * self.theta[1:]\n",
    "            \n",
    "                # Actualización GDX con momento\n",
    "                delta_theta = momentum * delta_theta - (1 - momentum) * lr * grad\n",
    "                self.theta += delta_theta\n",
    "        \n",
    "            # Ajuste adaptativo de learning rate\n",
    "            if loss_e > previous_loss * max_perf_inc:\n",
    "                self.theta = THETA_prev\n",
    "                lr *= lr_dec\n",
    "            elif loss_e < previous_loss:\n",
    "                lr *= lr_inc\n",
    "            \n",
    "            epoch_loss.append(loss_e)\n",
    "        \n",
    "            # Parada temprana\n",
    "            if abs(previous_loss - loss_e) < stopping_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {e}\")\n",
    "                break\n",
    "            \n",
    "            previous_loss = loss_e\n",
    "        \n",
    "            if verbose and e % show_step == 0:\n",
    "                print(f'Epoch: {e}, Loss: {loss_e:.6f}, LR: {lr:.6f}')\n",
    "            \n",
    "        return self.theta, batch_loss, epoch_loss\n",
    "                \n",
    "    def predict(self, A, threshold):\n",
    "        y_predicted = self._sigmoid(A, self.theta)  #make prediction\n",
    "        # Assign prediction to a class: \n",
    "        # if pred >= threshold then 1 else 0 and return as an array\n",
    "        y_predicted_cls = [1 if i >= threshold else 0 for i in y_predicted]\n",
    "        return np.array(y_predicted_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "798d5677-39f4-47ee-93f5-17d1e49f3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the data\n",
    "# data = np.loadtxt('admisiones_dataset.txt',delimiter=',')\n",
    "# inputs = data[:,0:2]\n",
    "# idx = 2-data[:,2] #restamos el 1 para establecer el categorico, adminitivos - 1 no admitivos - 0 \n",
    "# targets = np.array(idx, dtype=int)     # codificacion categorica\n",
    "# # targets = one_hot_encode(labels)      # one hot encode to classlabel\n",
    "\n",
    "\n",
    "# Leer datos desde archivo cancer.dat\n",
    "try:\n",
    "    # Intenta leer como archivo de texto\n",
    "    data = np.loadtxt('cancer_dataset.dat', delimiter=',')\n",
    "except:\n",
    "    # Intenta leer como binario si falla\n",
    "    data = np.fromfile('cancer_dataset.dat', dtype=np.float32)\n",
    "    data = data.reshape((-1, 31))  # Ajustar según la estructura de tus datos\n",
    "\n",
    "# Procesamiento de datos\n",
    "inputs = data[:, :-1]\n",
    "targets = data[:, -1].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417d0186-ad3b-4681-9057-819c0aa169e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data\n",
    "# x_train,x_test,y_train,y_test = train_test_split(inputs,targets,test_size=0.40,random_state=1234) # test_size genreta entrenamiento y prueba \n",
    "\n",
    "# División de datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs, targets, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11561f71-089f-435f-af81-2b4f72d0465f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83033211-6e0d-4738-8fa8-e8dcdb709bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matrices de disenio, \n",
    "A_train = np.c_[np.ones(len(x_train)), x_train]\n",
    "A_test  = np.c_[np.ones(len(x_test)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbf72d3-05a0-4991-95b5-db88808bc31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros para el constructor\n",
    "\n",
    "#minilot4es\n",
    "# Hiperparámetros GDX (SOLO para fit())\n",
    "# lambda_param = 0.1  # Regularización L2\n",
    "# fit_params = {\n",
    "#     'learning_rate': 0.01,\n",
    "#     'momentum': 0.95,\n",
    "#     'lr_dec': 0.5,\n",
    "#     'lr_inc': 1.05,\n",
    "#     'max_perf_inc': 1.04,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 64,\n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-6,\n",
    "#     'verbose': True\n",
    "#     # ELIMINAR 'lambda_param' de aquí porque no pertenece a fit()\n",
    "# }\n",
    "\n",
    "\n",
    "#online\n",
    "# lambda_param = 0.1  # Regularización L2\n",
    "# fit_params = {\n",
    "#     'learning_rate': 0.01,\n",
    "#     'momentum': 0.95,\n",
    "#     'lr_dec': 0.5,\n",
    "#     'lr_inc': 1.05,\n",
    "#     'max_perf_inc': 1.04,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 1,\n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-6,\n",
    "#     'verbose': True\n",
    "#     # ELIMINAR 'lambda_param' de aquí porque no pertenece a fit()\n",
    "# }\n",
    "\n",
    "\n",
    "#online\n",
    "lambda_param = 0.1  # Regularización L2\n",
    "fit_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'momentum': 0.95,\n",
    "    'lr_dec': 0.5,\n",
    "    'lr_inc': 1.05,\n",
    "    'max_perf_inc': 1.04,\n",
    "    'epochs': 300,\n",
    "    'batch_size': len(y_train),\n",
    "    'show_step': 50,\n",
    "    'stopping_threshold': 1e-4,\n",
    "    'verbose': True\n",
    "    # ELIMINAR 'lambda_param' de aquí porque no pertenece a fit()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2cb828a-4d54-4a44-86f5-3d37c8e15bd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'delta_theta' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Build and fit best LR model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# alpha = 0.01 #lr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# maxEpochs = 5000\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Entrenamiento del modelo\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m Logistic_Regression_GDX(lambda_param\u001b[38;5;241m=\u001b[39mlambda_param)  \u001b[38;5;66;03m# lambda_param aquí\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m theta, batch_loss, epoch_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(A_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "Cell \u001b[1;32mIn[5], line 101\u001b[0m, in \u001b[0;36mLogistic_Regression_GDX.fit\u001b[1;34m(self, A, y, learning_rate, momentum, lr_dec, lr_inc, max_perf_inc, epochs, batch_size, show_step, stopping_threshold, verbose)\u001b[0m\n\u001b[0;32m     98\u001b[0m     grad[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlambda_param\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(A_batch)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Actualización GDX con momento\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     delta_theta \u001b[38;5;241m=\u001b[39m momentum \u001b[38;5;241m*\u001b[39m delta_theta \u001b[38;5;241m-\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m momentum) \u001b[38;5;241m*\u001b[39m lr \u001b[38;5;241m*\u001b[39m grad\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m delta_theta\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# Ajuste adaptativo de learning rate\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'delta_theta' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Build and fit best LR model\n",
    "# alpha = 0.01 #lr\n",
    "# maxEpochs = 5000\n",
    "# batch = 10 #minilotes\n",
    "# show = 500 #view\n",
    "\n",
    "# # Build model\n",
    "# log_model = Logistic_Regression()\n",
    "# # Fit Model\n",
    "# theta, batch_loss, epoch_loss = log_model.fit(A_train, y_train, learning_rate=alpha, \n",
    "#                                 epochs=maxEpochs, batch_size=batch, show_step = show, verbose=True)\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = Logistic_Regression_GDX(lambda_param=lambda_param)  # lambda_param aquí\n",
    "theta, batch_loss, epoch_loss = model.fit(A_train, y_train, **fit_params)  # sin lambda_param aquí"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a02458-0b52-41fd-bb27-fe14f88559e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicciones y evaluación\n",
    "train_pred = model.predict(A_train, 0.5)\n",
    "test_pred = model.predict(A_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1ddc0-9364-4d1a-8fa8-8ec5dffbe8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Modo de aprendizaje: {'Online' if fit_params['batch_size'] == 1 else 'Batch completo' if fit_params['batch_size'] >= len(y_train) else 'Mini-lotes'}\")\n",
    "print(f\"Tamaño de lote: {fit_params['batch_size']}\")\n",
    "print(f\"Regularización L2: lambda={lambda_param}\")\n",
    "print(f\"Épocas completadas: {len(epoch_loss)}/{fit_params['epochs']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a76437-bc5e-4506-ab8e-1e8d3dd59c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate accuracy\n",
    "# train_acc = accuracy(y_train, train_pred)\n",
    "# print(f'Accuracy on training set: {train_acc}')\n",
    "#resultados finales\n",
    "print(f'Accuracy: {accuracy(y_train, train_pred):.4f}')\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(confusion_matrix(y_train, train_pred))\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_train, train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089fcf0d-4bcf-4b7a-b52a-ce1d7c7d6c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate metrics - son de entrenamiento \n",
    "# cm_train = confusion_matrix(y_train, train_pred)\n",
    "# train_report = classification_report(y_train, train_pred)\n",
    "\n",
    "# print(\"Performance on training set:\\n\")\n",
    "# print(f'Confusion Matrix:\\n {cm_train}\\n')\n",
    "# print(f'Classification Report:\\n {train_report}')\n",
    "\n",
    "print(f'Accuracy: {accuracy(y_test, test_pred):.4f}')\n",
    "print(\"\\nMatriz de confusión:\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcd2268-1e45-493f-b36c-7c4396fb93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de pérdida\n",
    "# Gráfica de pérdida\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_loss, 'b-', linewidth=2)\n",
    "plt.title('Evolución de la Pérdida por Época')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida (Entropía Cruzada Binaria)')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_evolution.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad07d7-65b3-4a2c-a1ff-8cc26dae3476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d782a92-d75a-4ff6-9f6f-7dde330ec531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e98d4c-eba1-4a6a-8117-d3a079cbda2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
