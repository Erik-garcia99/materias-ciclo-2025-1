{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32eb9a44-0a1d-4ff0-afc3-8056457cec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #serpara los datos \n",
    "from sklearn.metrics import confusion_matrix, classification_report #estos los regulatado las claisigfacion y la matriz de confusion\n",
    "from sklearn.preprocessing import StandardScaler #normalizar los datos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fedb57-aa3c-4d52-bf89-41049ca5cf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def to_classlabel(z):\n",
    "    return z.argmax(axis = 1) #etiqeuta en la columna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc1f1f70-65df-4b01-a59e-5ab1cc699753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y):\n",
    "    n_class = np.unique(y).shape[0] #cunatas clases tiene Y \n",
    "    y_encode = np.zeros((y.shape[0], n_class)) #inicialzia la tabla, rengloes(instancias ) columnas numero de clases\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd24dcf8-4870-4d34-98df-b92856f102d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc #presicion \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4cc422-960e-49dd-830e-cd298913da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Logistic_Regression_AdamD():\n",
    "    \n",
    "    def __init__(self, lambda_param=0.01):\n",
    "        self.theta = None\n",
    "        self.lambda_param = lambda_param \n",
    "        \n",
    "    \n",
    "    def _sigmoid(self, A, theta):\n",
    "        yh = np.dot(A, theta)\n",
    "        yh = np.clip(yh, -500, 500)\n",
    "        return 1/(1 + np.exp(-yh))\n",
    "\n",
    "\n",
    "\n",
    "    #fiunciond e cosot binaria \n",
    "    \"\"\"def _loss(self, y, h):\n",
    "        '''\n",
    "        a really small value 'epsilon' is added to avoid \n",
    "        overflow and divison by zero error for log\n",
    "        loss = (-1/q) * sum(y * log(h) + (1-y) * log(1 - h))\n",
    "        where h = 1/(1 + e^(-yh))\n",
    "        '''\n",
    "        #el epsislon funciona por si la hipotrsis nos da 0 (log -> logaritmo natural )\n",
    "        epsilon = 1e-5\n",
    "        # modificacion \n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        # Pérdida con regularización L2\n",
    "        reg_term = (self.lambda_param/(2*len(y))) * np.sum(self.theta[1:]**2)\n",
    "        los = (-1/len(y)) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h+epsilon)) + reg_term\n",
    "        return los\"\"\"\n",
    "    \n",
    "    def _loss(self, y, h, theta):\n",
    "       \n",
    "        epsilon = 1e-5\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        n = len(y)\n",
    "        loss = - (1/n) * np.sum(y * np.log(h) + (1-y) * np.log(1-h))\n",
    "        reg_term = (self.lambda_param/(2*n)) * np.sum(theta[1:]**2)\n",
    "        return loss + reg_term\n",
    "    \n",
    "    def gradient(self, A, y, h, theta):\n",
    "        n = len(y)\n",
    "        error = h - y\n",
    "        grad = (1/n) * np.dot(A.T, error)\n",
    "        grad[1:] += (self.lambda_param/n) * theta[1:]\n",
    "        return grad\n",
    "\n",
    "    def fit(self, A, y, learning_rate=0.001, beta1=0.9, beta2=0.999, \n",
    "            epsilon=1e-8, epochs=100, batch_size=32, show_step=10, \n",
    "            stopping_threshold=1e-6, verbose=False):\n",
    "        ''' \n",
    "        Entrenamiento con AdamD \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        A: matriz de diseño (incluye columna de unos)\n",
    "        y: vector de etiquetas (0 o 1)\n",
    "        learning_rate: tasa de aprendizaje (alpha)\n",
    "        beta1, beta2: parámetros de momento\n",
    "        epsilon: constante de estabilidad numérica\n",
    "        epochs: número máximo de épocas\n",
    "        batch_size: tamaño del minilote\n",
    "        show_step: cada cuántas épocas mostrar información\n",
    "        stopping_threshold: umbral de parada temprana (cambio en pérdida)\n",
    "        verbose: si True, imprime progreso\n",
    "        '''\n",
    "        n_obs, n_features = A.shape\n",
    "        self.theta = np.random.randn(n_features) * 0.01\n",
    "        m = np.zeros(n_features)\n",
    "        v = np.zeros(n_features)\n",
    "        t = 0\n",
    "        \n",
    "        epoch_losses = []\n",
    "        previous_loss = np.inf\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            permutation = np.random.permutation(n_obs)\n",
    "            A_shuffled = A[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            n_batches = n_obs // batch_size\n",
    "            residual = n_obs % batch_size\n",
    "            total_batches = n_batches + (1 if residual != 0 else 0)\n",
    "            \n",
    "            for batch_idx in range(total_batches):\n",
    "                t += 1\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                if batch_idx == total_batches - 1 and residual != 0:\n",
    "                    end = start + residual\n",
    "                \n",
    "                A_batch = A_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                h_batch = self._sigmoid(A_batch, self.theta)\n",
    "                loss_batch = self._loss(y_batch, h_batch, self.theta)\n",
    "                epoch_loss += loss_batch * len(y_batch)\n",
    "                \n",
    "                grad = self.gradient(A_batch, y_batch, h_batch, self.theta)\n",
    "                \n",
    "                m = beta1 * m + (1 - beta1) * grad\n",
    "                v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "                \n",
    "                alpha_t = learning_rate * np.sqrt(1 - beta2**t)\n",
    "                \n",
    "                self.theta -= alpha_t * m / (np.sqrt(v) + epsilon)\n",
    "            \n",
    "            epoch_loss /= n_obs\n",
    "            epoch_losses.append(epoch_loss)\n",
    "\n",
    "            if epoch > 10 and abs(previous_loss - epoch_loss) < stopping_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            previous_loss = epoch_loss\n",
    "            \n",
    "            if verbose and epoch % show_step == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {epoch_loss:.3e}')\n",
    "        \n",
    "        return self.theta, epoch_losses\n",
    "    \n",
    "    def predict(self, A, threshold=0.5):\n",
    "        y_predicted = self._sigmoid(A, self.theta)\n",
    "        # Usar vectorización en lugar de comprensión de lista\n",
    "        return (y_predicted >= threshold).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e87c7a18-634c-45b3-a416-d7826a5dff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Logistic_Regression_AdamD_Multiclass():\n",
    "    def __init__(self, lambda_param=0.01, num_classes=6):\n",
    "        self.models = []\n",
    "        self.losses = []\n",
    "        self.lambda_param = lambda_param\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def fit(self, A, y, **fit_params):\n",
    "        self.models = []\n",
    "        self.losses = []\n",
    "        \n",
    "        for i in range(self.num_classes):\n",
    "            y_binary = np.where(y == i, 1, 0)\n",
    "            \n",
    "            positive_indices = np.where(y_binary == 1)[0]\n",
    "            negative_indices = np.where(y_binary == 0)[0]\n",
    "            \n",
    "            n_pos = len(positive_indices)\n",
    "            \n",
    "            if n_pos > 0:\n",
    "                neg_selected = np.random.choice(negative_indices, size=n_pos, replace=False)\n",
    "                balanced_indices = np.concatenate([positive_indices, neg_selected])\n",
    "                np.random.shuffle(balanced_indices)\n",
    "                \n",
    "                A_balanced = A[balanced_indices]\n",
    "                y_balanced = y_binary[balanced_indices]\n",
    "            else:\n",
    "                print(f\"Advertencia: Clase {i} tiene 0 muestras positivas\")\n",
    "                A_balanced = A\n",
    "                y_balanced = y_binary\n",
    "                \n",
    "            model = Logistic_Regression_AdamD(lambda_param=self.lambda_param)\n",
    "            theta, epoch_losses = model.fit(A_balanced, y_balanced, **fit_params)\n",
    "            self.models.append(model)\n",
    "            self.losses.append(epoch_losses)\n",
    "    \n",
    "    def predict_proba(self, A):\n",
    "        probas = []\n",
    "        for model in self.models:\n",
    "            proba = model._sigmoid(A, model.theta)\n",
    "            probas.append(proba)\n",
    "        return np.array(probas).T\n",
    "    \n",
    "    def predict(self, A):\n",
    "        probas = self.predict_proba(A)\n",
    "        return np.argmax(probas, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ae4f14-c02b-4dee-ac69-4f2b6e8fa06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erikG\\AppData\\Local\\Temp\\ipykernel_12696\\3446328204.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inputs[33] = inputs[33].fillna(age_mean)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Read the data\n",
    "# data = np.loadtxt('admisiones_dataset.txt',delimiter=',')\n",
    "# inputs = data[:,0:2]\n",
    "# idx = 2-data[:,2] #restamos el 1 para establecer el categorico, adminitivos - 1 no admitivos - 0 \n",
    "# targets = np.array(idx, dtype=int)     # codificacion categorica\n",
    "# # targets = one_hot_encode(labels)      # one hot encode to classlabel\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('dermatology.dat', header=None, na_values='?', delimiter=r'\\s+')\n",
    "    \n",
    "    inputs = df.iloc[:, 0:34]\n",
    "    targets = df.iloc[:, 34]\n",
    "    \n",
    "    age_mean = inputs[33].mean()\n",
    "    inputs[33] = inputs[33].fillna(age_mean)\n",
    "    \n",
    "    inputs = inputs.astype(float)\n",
    "    targets = targets.astype(int) - 1  # Convertir clases [1-6] a [0-5]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al leer el archivo: {e}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e989b8f0-94d3-4226-9ca2-2b9cf013600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data\n",
    "# x_train,x_test,y_train,y_test = train_test_split(inputs,targets,test_size=0.40,random_state=1234) # test_size genreta entrenamiento y prueba \n",
    "\n",
    "# División de datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs, targets, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b04a5bb6-b5dd-43de-8b9b-c38ca8115db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1fbfb9b-db46-4289-a91d-b871e0165e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrices de disenio, \n",
    "A_train = np.c_[np.ones(len(x_train)), x_train]\n",
    "A_test  = np.c_[np.ones(len(x_test)), x_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b418adf5-88be-4732-85ef-47c814d5fb15",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3548764846.py, line 35)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 35\u001b[1;36m\u001b[0m\n\u001b[1;33m    lambda_param = 0.01\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parámetros para el constructor\n",
    "\n",
    "#minilot4es\n",
    "\n",
    "# lambda_param = 0.01  \n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'beta1': 0.9,\n",
    "#     'beta2': 0.999,\n",
    "#     'epsilon': 1e-8,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 128, \n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-6,\n",
    "#     'verbose': True\n",
    "# }\n",
    "\n",
    "\n",
    "#online\n",
    "# lambda_param = 0.01 \n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-3,\n",
    "#     'beta1': 0.9,\n",
    "#     'beta2': 0.999,\n",
    "#     'epsilon': 1e-8,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 1,  \n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-4,\n",
    "#     'verbose': True\n",
    "# }\n",
    "\n",
    "\n",
    "#lote completo\n",
    "lambda_param = 0.01  \n",
    "fit_params = {\n",
    "    'learning_rate': 1e-3,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'epsilon': 1e-8,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': len(y_train),  \n",
    "    'show_step': 100,\n",
    "    'stopping_threshold': 1e-4,\n",
    "    'verbose': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3bb5e4-4370-45e3-8312-1f84237e558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Build and fit best LR model\n",
    "# alpha = 0.01 #lr\n",
    "# maxEpochs = 5000\n",
    "# batch = 10 #minilotes\n",
    "# show = 500 #view\n",
    "\n",
    "# # Build model\n",
    "# log_model = Logistic_Regression()\n",
    "# # Fit Model\n",
    "# theta, batch_loss, epoch_loss = log_model.fit(A_train, y_train, learning_rate=alpha, \n",
    "#                                 epochs=maxEpochs, batch_size=batch, show_step = show, verbose=True)\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = Logistic_Regression_AdamD_Multiclass(lambda_param=lambda_param, num_classes=6)\n",
    "model.fit(A_train, y_train, **fit_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce17a8-b84d-420f-8db8-a7dc1404045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicciones y evaluación\n",
    "# Predicciones\n",
    "train_pred = model.predict(A_train)\n",
    "test_pred = model.predict(A_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb8f99-edd7-4d50-af70-05b9ade71494",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate accuracy\n",
    "# train_acc = accuracy(y_train, train_pred)\n",
    "# print(f'Accuracy on training set: {train_acc}')\n",
    "#resultados finales\n",
    "# Resultados en entrenamiento\n",
    "print(\"\\nrendimiento en entrenamiento (multiclase - AdamD):\")\n",
    "print(f'Accuracy: {accuracy(y_train, train_pred):.4f}')\n",
    "print(\"\\nmatriz de confusion:\")\n",
    "print(confusion_matrix(y_train, train_pred))\n",
    "print(\"\\nreporte de clasificacion:\")\n",
    "print(classification_report(y_train, train_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d14fe5-5a3e-413b-835e-6bb52d1f91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate metrics - son de entrenamiento \n",
    "# cm_train = confusion_matrix(y_train, train_pred)\n",
    "# train_report = classification_report(y_train, train_pred)\n",
    "\n",
    "# print(\"Performance on training set:\\n\")\n",
    "# print(f'Confusion Matrix:\\n {cm_train}\\n')\n",
    "# print(f'Classification Report:\\n {train_report}')\n",
    "\n",
    "# Resultados en prueba\n",
    "print(\"\\nrendimiento en prueba (multiclase - AdamD):\")\n",
    "print(f'Accuracy: {accuracy(y_test, test_pred):.4f}')\n",
    "print(\"\\nmatriz de confusion:\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(\"\\nreporte de clasificacion:\")\n",
    "print(classification_report(y_test, test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123367d-6384-4d0a-9888-2763e9b6c1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfica de pérdida\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, loss_list in enumerate(model.losses):\n",
    "    plt.plot(loss_list, label=f'Clase {i}')\n",
    "plt.title('evolución de la Perdida por epoca (AdamD Multiclase)')\n",
    "plt.xlabel('epoca')\n",
    "plt.ylabel('perdida (Entropia cruzada binaria)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb940fa2-0125-4779-9381-95f837a1d3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
