{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328a2f34-98eb-4396-bf0d-2ab6dc52340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split #serpara los datos \n",
    "from sklearn.metrics import confusion_matrix, classification_report #estos los regulatado las claisigfacion y la matriz de confusion\n",
    "from sklearn.preprocessing import StandardScaler #normalizar los datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70788833-cf05-4117-a94f-4c8afaae3515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_classlabel(z):\n",
    "    return z.argmax(axis = 1) #etiqeuta en la columna\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3928747-967f-4d9b-afd1-8ad9910b16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y susecion de numeros, \n",
    "def one_hot_encode(y):\n",
    "    n_class = np.unique(y).shape[0] #cunatas clases tiene Y \n",
    "    y_encode = np.zeros((y.shape[0], n_class)) #inicialzia la tabla, rengloes(instancias ) columnas numero de clases\n",
    "    for idx, val in enumerate(y):\n",
    "        y_encode[idx, val] = 1.0\n",
    "    return y_encode\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0332ace7-9325-4edf-85e5-2fb237fa139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    acc = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return acc #presicion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec908dd-bae0-451d-a8d3-499ca476612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression_AdamD():\n",
    "    \n",
    "    def __init__(self, lambda_param=0.01):\n",
    "        self.theta = None\n",
    "        self.lambda_param = lambda_param  # Parámetro de regularización L2\n",
    "        \n",
    "    \n",
    "    def _sigmoid(self, A, theta):\n",
    "        yh = np.dot(A, theta)\n",
    "        # Añadir clip para estabilidad numérica\n",
    "        yh = np.clip(yh, -500, 500)\n",
    "        return 1/(1 + np.exp(-yh))\n",
    "\n",
    "\n",
    "\n",
    "    #fiunciond e cosot binaria \n",
    "    \"\"\"def _loss(self, y, h):\n",
    "        '''\n",
    "        a really small value 'epsilon' is added to avoid \n",
    "        overflow and divison by zero error for log\n",
    "        loss = (-1/q) * sum(y * log(h) + (1-y) * log(1 - h))\n",
    "        where h = 1/(1 + e^(-yh))\n",
    "        '''\n",
    "        #el epsislon funciona por si la hipotrsis nos da 0 (log -> logaritmo natural )\n",
    "        epsilon = 1e-5\n",
    "        # modificacion \n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        # Pérdida con regularización L2\n",
    "        reg_term = (self.lambda_param/(2*len(y))) * np.sum(self.theta[1:]**2)\n",
    "        los = (-1/len(y)) * np.sum(y * np.log(h + epsilon) + (1-y) * np.log(1-h+epsilon)) + reg_term\n",
    "        return los\"\"\"\n",
    "    \n",
    "    def _loss(self, y, h, theta):\n",
    "        ''' Función de pérdida: entropía cruzada binaria con regularización L2 '''\n",
    "        epsilon = 1e-5\n",
    "        h = np.clip(h, epsilon, 1 - epsilon)\n",
    "        n = len(y)\n",
    "        # Término de entropía cruzada\n",
    "        loss = - (1/n) * np.sum(y * np.log(h) + (1-y) * np.log(1-h))\n",
    "        # Término de regularización L2 (excluimos el sesgo, theta[0])\n",
    "        reg_term = (self.lambda_param/(2*n)) * np.sum(theta[1:]**2)\n",
    "        return loss + reg_term\n",
    "    \n",
    "    def gradient(self, A, y, h, theta):\n",
    "        ''' Gradiente de la pérdida con regularización L2 '''\n",
    "        n = len(y)\n",
    "        error = h - y\n",
    "        grad = (1/n) * np.dot(A.T, error)\n",
    "        # Regularización L2 (excluyendo el sesgo)\n",
    "        grad[1:] += (self.lambda_param/n) * theta[1:]\n",
    "        return grad\n",
    "\n",
    "    def fit(self, A, y, learning_rate=0.001, beta1=0.9, beta2=0.999, \n",
    "            epsilon=1e-8, epochs=100, batch_size=32, show_step=10, \n",
    "            stopping_threshold=1e-6, verbose=False):\n",
    "        ''' \n",
    "        Entrenamiento con AdamD \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        A: matriz de diseño (incluye columna de unos)\n",
    "        y: vector de etiquetas (0 o 1)\n",
    "        learning_rate: tasa de aprendizaje (alpha)\n",
    "        beta1, beta2: parámetros de momento\n",
    "        epsilon: constante de estabilidad numérica\n",
    "        epochs: número máximo de épocas\n",
    "        batch_size: tamaño del minilote\n",
    "        show_step: cada cuántas épocas mostrar información\n",
    "        stopping_threshold: umbral de parada temprana (cambio en pérdida)\n",
    "        verbose: si True, imprime progreso\n",
    "        '''\n",
    "        n_obs, n_features = A.shape\n",
    "        # Inicialización aleatoria como en GDX\n",
    "        self.theta = np.random.randn(n_features) * 0.01\n",
    "        m = np.zeros(n_features)\n",
    "        v = np.zeros(n_features)\n",
    "        t = 0\n",
    "        \n",
    "        epoch_losses = []\n",
    "        previous_loss = np.inf\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Barajar los datos\n",
    "            permutation = np.random.permutation(n_obs)\n",
    "            A_shuffled = A[permutation]\n",
    "            y_shuffled = y[permutation]\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            n_batches = n_obs // batch_size\n",
    "            residual = n_obs % batch_size\n",
    "            total_batches = n_batches + (1 if residual != 0 else 0)\n",
    "            \n",
    "            for batch_idx in range(total_batches):\n",
    "                t += 1\n",
    "                start = batch_idx * batch_size\n",
    "                end = start + batch_size\n",
    "                # Manejo robusto de batch residual como en GDX\n",
    "                if batch_idx == total_batches - 1 and residual != 0:\n",
    "                    end = start + residual\n",
    "                \n",
    "                A_batch = A_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                \n",
    "                h_batch = self._sigmoid(A_batch, self.theta)\n",
    "                loss_batch = self._loss(y_batch, h_batch, self.theta)\n",
    "                epoch_loss += loss_batch * len(y_batch)\n",
    "                \n",
    "                grad = self.gradient(A_batch, y_batch, h_batch, self.theta)\n",
    "                \n",
    "                # Actualización AdamD\n",
    "                m = beta1 * m + (1 - beta1) * grad\n",
    "                v = beta2 * v + (1 - beta2) * (grad ** 2)\n",
    "                \n",
    "                # Corrección de bias como en el algoritmo original\n",
    "                alpha_t = learning_rate * np.sqrt(1 - beta2**t)\n",
    "                \n",
    "                self.theta -= alpha_t * m / (np.sqrt(v) + epsilon)\n",
    "            \n",
    "            epoch_loss /= n_obs\n",
    "            epoch_losses.append(epoch_loss)\n",
    "            \n",
    "            # Parada temprana mejorada\n",
    "            if epoch > 10 and abs(previous_loss - epoch_loss) < stopping_threshold:\n",
    "                if verbose:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "            previous_loss = epoch_loss\n",
    "            \n",
    "            if verbose and epoch % show_step == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {epoch_loss:.6f}')\n",
    "        \n",
    "        return self.theta, epoch_losses\n",
    "    \n",
    "    def predict(self, A, threshold=0.5):\n",
    "        y_predicted = self._sigmoid(A, self.theta)\n",
    "        # Usar vectorización en lugar de comprensión de lista\n",
    "        return (y_predicted >= threshold).astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d4503e-d602-4e8d-82c8-b2ad40fd7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Read the data\n",
    "# data = np.loadtxt('admisiones_dataset.txt',delimiter=',')\n",
    "# inputs = data[:,0:2]\n",
    "# idx = 2-data[:,2] #restamos el 1 para establecer el categorico, adminitivos - 1 no admitivos - 0 \n",
    "# targets = np.array(idx, dtype=int)     # codificacion categorica\n",
    "# # targets = one_hot_encode(labels)      # one hot encode to classlabel\n",
    "\n",
    "\n",
    "# Leer datos desde archivo .dat\n",
    "try:\n",
    "    # Intenta leer como archivo de texto\n",
    "    data = np.loadtxt('cancer_dataset.dat', delimiter=',')\n",
    "except:\n",
    "    # Intenta leer como binario si falla\n",
    "    data = np.fromfile('cancer_dataset.dat', dtype=np.float32)\n",
    "    data = data.reshape((-1, 31))  # Ajustar según la estructura de tus datos\n",
    "\n",
    "# Procesamiento de datos\n",
    "inputs = data[:, :-1]\n",
    "targets = data[:, -1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "309a429d-770f-40dc-9d0d-df846e5be72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data\n",
    "# x_train,x_test,y_train,y_test = train_test_split(inputs,targets,test_size=0.40,random_state=1234) # test_size genreta entrenamiento y prueba \n",
    "\n",
    "# División de datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    inputs, targets, test_size=0.3, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20f85692-aee8-409c-b736-a4a7e5915bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b4d774-342b-45af-96a3-c8a313de6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#matrices de disenio, \n",
    "A_train = np.c_[np.ones(len(x_train)), x_train]\n",
    "A_test  = np.c_[np.ones(len(x_test)), x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd7b622-d906-4c63-9023-54d94ef2a839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parámetros para el constructor\n",
    "\n",
    "#minilot4es\n",
    "# Hiperparámetros para AdamD\n",
    "lambda_param = 0.01  # Regularización L2\n",
    "fit_params = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'beta1': 0.9,\n",
    "    'beta2': 0.999,\n",
    "    'epsilon': 1e-8,\n",
    "    'epochs': 1000,\n",
    "    'batch_size': 128,  # Tamaño de minilote\n",
    "    'show_step': 100,\n",
    "    'stopping_threshold': 1e-4,\n",
    "    'verbose': True\n",
    "}\n",
    "\n",
    "\n",
    "#online\n",
    "# lambda_param = 0.01  # Regularización L2\n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'beta1': 0.9,\n",
    "#     'beta2': 0.999,\n",
    "#     'epsilon': 1e-8,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': 1,  \n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-4,\n",
    "#     'verbose': True\n",
    "# }\n",
    "\n",
    "\n",
    "# #lote completo\n",
    "# lambda_param = 0.01  # Regularización L2\n",
    "# fit_params = {\n",
    "#     'learning_rate': 1e-4,\n",
    "#     'beta1': 0.9,\n",
    "#     'beta2': 0.999,\n",
    "#     'epsilon': 1e-8,\n",
    "#     'epochs': 1000,\n",
    "#     'batch_size': len(y_train),  \n",
    "#     'show_step': 100,\n",
    "#     'stopping_threshold': 1e-4,\n",
    "#     'verbose': True\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a46439d5-70e6-4d82-8f6d-72379691c566",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lambda_param' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# # Build and fit best LR model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# alpha = 0.01 #lr\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# maxEpochs = 5000\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Entrenamiento del modelo\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m Logistic_Regression_AdamD(lambda_param\u001b[38;5;241m=\u001b[39mlambda_param)\n\u001b[0;32m     16\u001b[0m theta, epoch_losses \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(A_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lambda_param' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Build and fit best LR model\n",
    "# alpha = 0.01 #lr\n",
    "# maxEpochs = 5000\n",
    "# batch = 10 #minilotes\n",
    "# show = 500 #view\n",
    "\n",
    "# # Build model\n",
    "# log_model = Logistic_Regression()\n",
    "# # Fit Model\n",
    "# theta, batch_loss, epoch_loss = log_model.fit(A_train, y_train, learning_rate=alpha, \n",
    "#                                 epochs=maxEpochs, batch_size=batch, show_step = show, verbose=True)\n",
    "\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model = Logistic_Regression_AdamD(lambda_param=lambda_param)\n",
    "theta, epoch_losses = model.fit(A_train, y_train, **fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac6691f-aecf-427d-b7dd-1db428a43008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Predicciones y evaluación\n",
    "# Predicciones\n",
    "train_pred = model.predict(A_train)\n",
    "test_pred = model.predict(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26620f4-2d04-4856-99bf-c82b22305b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluación\n",
    "print(f\"Modo de aprendizaje: Minilotes (tamaño {fit_params['batch_size']})\")\n",
    "print(f\"Regularización L2: lambda={lambda_param}\")\n",
    "print(f\"Épocas completadas: {len(epoch_losses)}/{fit_params['epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fc1b16-540f-4070-aba5-1052e13359bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Calculate accuracy\n",
    "# train_acc = accuracy(y_train, train_pred)\n",
    "# print(f'Accuracy on training set: {train_acc}')\n",
    "#resultados finales\n",
    "# Resultados en entrenamiento\n",
    "print('\\nResultados en entrenamiento:')\n",
    "print(f'Accuracy: {accuracy(y_train, train_pred):.4f}')\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_train, train_pred))\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_train, train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8ca13-6950-495f-80ae-896a028c5240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate metrics - son de entrenamiento \n",
    "# cm_train = confusion_matrix(y_train, train_pred)\n",
    "# train_report = classification_report(y_train, train_pred)\n",
    "\n",
    "# print(\"Performance on training set:\\n\")\n",
    "# print(f'Confusion Matrix:\\n {cm_train}\\n')\n",
    "# print(f'Classification Report:\\n {train_report}')\n",
    "\n",
    "# Resultados en prueba\n",
    "print('\\nResultados en prueba:')\n",
    "print(f'Accuracy: {accuracy(y_test, test_pred):.4f}')\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion_matrix(y_test, test_pred))\n",
    "print(\"Reporte de clasificación:\")\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e4f317-d7ac-4b4a-8e4c-69f25dd1c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Gráfica de pérdida\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epoch_losses, 'b-', linewidth=2)\n",
    "plt.title('Evolución de la Pérdida por Época (AdamD)')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida (Entropía Cruzada Binaria)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb4aa35-294c-4652-b727-c6303ecf6cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ac2e6-e3b4-4c1e-b58f-265e26be30b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fef087-b1f8-4313-8c82-7c636b94bc8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
